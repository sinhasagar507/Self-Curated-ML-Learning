{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0BFRaytRJbZE",
    "outputId": "0b597ccd-3be9-4ffb-aa64-5438ff3b8ef1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 1.8 MB 33.5 MB/s \n",
      "\u001b[K     |████████████████████████████████| 181 kB 58.1 MB/s \n",
      "\u001b[K     |████████████████████████████████| 145 kB 70.3 MB/s \n",
      "\u001b[K     |████████████████████████████████| 63 kB 2.0 MB/s \n",
      "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "# Weights and biases\n",
    "# !pip install wandb -qqq\n",
    "# import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6vQEUB9eJbWL"
   },
   "outputs": [],
   "source": [
    "# Login into the account\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B68XuxjoReNY"
   },
   "source": [
    "# Play with every model, parameter and the following dataset - thrice "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wVyWNadAJbS2",
    "outputId": "6f4faf7d-f085-466c-8174-1abee139236f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DSAforDSA is a portal for learning DSA.\n",
      "Hello, My name is Sagar and I'm 22 years old.\n",
      " June 17, 2022\n"
     ]
    }
   ],
   "source": [
    "# Python3 program introducing f-string. F-strings are cooler\n",
    "val = 'DSA'\n",
    "print(f\"{val}for{val} is a portal for learning {val}.\")\n",
    " \n",
    " \n",
    "name = 'Sagar'\n",
    "age = 22\n",
    "print(f\"Hello, My name is {name} and I'm {age} years old.\")\n",
    "\n",
    "# Importing datetime\n",
    "import datetime\n",
    "today = datetime.datetime.today()\n",
    "print(f\"{today: %B %d, %Y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oHe0LxeJJbPy",
    "outputId": "c936ace9-4037-4601-b341-e161fc8c6e42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-13 18:01:45.834234\n"
     ]
    }
   ],
   "source": [
    "print(today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "uWKm9VJlmS_S"
   },
   "outputs": [],
   "source": [
    "# Importing the requisite libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Import Pytorch. Study all of these four modules extensively \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision \n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h-H3g-h6mgAu",
    "outputId": "7204533b-51bd-438c-f5c0-13eaae36c8d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.5013, 1.7906],\n",
      "        [0.3806, 0.6719]])\n",
      "\n",
      "tensor([[0.8853, 0.7091, 0.4316, 0.5594],\n",
      "        [0.9416, 0.1178, 0.3512, 0.4684],\n",
      "        [0.4643, 0.1076, 0.3908, 0.9738],\n",
      "        [0.2501, 0.2615, 0.8101, 0.2507]])\n",
      "torch.Size([2, 8])\n",
      "\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "[1. 1. 1. 1. 1.]\n",
      "\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "# Random initialization \n",
    "# High bias refers to high error on training set. High variance refers to high error on test set\n",
    "x=torch.rand(2, 2)\n",
    "y=torch.rand(2, 2)\n",
    "\n",
    "z=x*y\n",
    "z=torch.div(x, y)\n",
    "print(z)\n",
    "print()\n",
    "\n",
    "x=torch.rand(4, 4)\n",
    "print(x)\n",
    "y=x.view(-1, 8) \n",
    "print(y.size())\n",
    "print()\n",
    "\n",
    "a=torch.ones(5)\n",
    "print(a)\n",
    "b=a.numpy()\n",
    "print(b)\n",
    "print()\n",
    "\n",
    "# Adding 1 to the initial tensor will modify the derived tensor or numpy array as well.\n",
    "a.add_(1)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2xci7j2_mf9O",
    "outputId": "29d8b7e2-3a67-407e-e81f-64e44ddb44d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.])\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU availibility \n",
    "if torch.cuda.is_available():\n",
    "  device=torch.device(\"cuda\")\n",
    "  x=torch.ones(5, device=device)\n",
    "  y=torch.ones(5, device=device)\n",
    "  z=x+y\n",
    "  z=z.to(\"cpu\")\n",
    "  print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVspqWtJ2WHe"
   },
   "source": [
    "# Autograd Package for Gradient Computation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "83SkIrXEmf6b",
    "outputId": "308a2783-87dd-4807-8d43-ec5f32e63c20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.9401,  0.0035, -1.0927], requires_grad=True)\n",
      "tensor([2.9401, 2.0035, 0.9073], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# When writing requires_grad=True, we explicitly specify that gradient computations can occur WRT this variable. Enable gradient computations WRT this variable\n",
    "x=torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "y=x+2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c7r3OZxpmf3w",
    "outputId": "e5f3ecdb-f808-49a1-cdcf-67073d3f9efb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.1760e+00, 8.0141e+00, 3.6293e-03])\n"
     ]
    }
   ],
   "source": [
    "z=y*y*2\n",
    "v=torch.tensor([0.1, 1.0, 0.001], dtype=torch.float32) \n",
    "z.backward(v) # Computes dz/dx at this particular value\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oAm0KyOvmf1I",
    "outputId": "4e047a80-6a6e-4612-cbc3-e6f667126a49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([6., 6., 6., 6.])\n",
      "tensor([9., 9., 9., 9.])\n"
     ]
    }
   ],
   "source": [
    "#Following are the three ways to initialize a vector without enabling gradient computation\n",
    "# x.requires_grad_(false)\n",
    "# x.detach()\n",
    "# with torch.no_grad():\n",
    "#   loop\n",
    "x1=torch.randn(3, requires_grad=False) \n",
    "\n",
    "\n",
    "#Understand how this is working\n",
    "weights=torch.ones(4, requires_grad=True)\n",
    "for epoch in range(3):\n",
    "  model_output=(weights*3).sum()\n",
    "\n",
    "  # Compute Gradient and print the output \n",
    "  model_output.backward()\n",
    "  print(weights.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1d1QOaIDmfub",
    "outputId": "5b9a24ef-c17c-471e-ec04-2ec7e29438fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<PowBackward0>)\n",
      "tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "# Basic forward and backward pass\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "# Loss is calculated WRT weights\n",
    "w = torch.tensor(1.0)\n",
    "\n",
    "# Forward pass and loss computation\n",
    "y_hat = w*x\n",
    "loss = (y_hat-y)**2\n",
    "print(loss)\n",
    "\n",
    "# Backward loss\n",
    "loss.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DO3I-FRntwtn"
   },
   "source": [
    "## Linear Regression with Gradient Computation in PyTorch - Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xQg5DFmv6p92",
    "outputId": "dc419027-3c31-4ee8-8c1e-f5c7c2d36438"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "Weights=1.2 Loss=30.0\n",
      "Weights=1.9918080282211301 Loss=0.0031457357108592987\n",
      "Weights=1.9999160599708554 Loss=3.297340072094812e-07\n",
      "Weights=1.9999991369247434 Loss=3.531397396727698e-11\n",
      "Weights=1.9999999952316283 Loss=0.0\n"
     ]
    }
   ],
   "source": [
    "# Update weights\n",
    "# Prediction, gradient computation, loss computation and parameter updates\n",
    "# Gradients and torch\n",
    "\n",
    "# Training samples\n",
    "X = np.array([1, 2, 3, 4], dtype=np.float32)\n",
    "Y = np.array([2, 4, 6, 8], dtype=np.float32)\n",
    "\n",
    "w = 0.0\n",
    "\n",
    "# Model prediction\n",
    "def forward(x):\n",
    "  return w*x\n",
    "\n",
    "# Loss = MSE\n",
    "def loss(y, y_predicted):\n",
    "  return((y_predicted - y)**2).mean()\n",
    "\n",
    "# gradient\n",
    "# MSE = 1/N * (w*x - y) ** 2\n",
    "# dJ/dW = 1/N * 2 * (w* x -y)\n",
    "\n",
    "def gradient(x, y, y_predicted):\n",
    "  return np.dot(2*x, y_predicted - y).mean()\n",
    "\n",
    "print(forward(5.0))\n",
    "\n",
    "#Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 25\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "\n",
    "  # Prediction = forward pass\n",
    "  y_preds = forward(X)\n",
    "\n",
    "  # Calculate loss\n",
    "  l = loss(Y, y_preds)\n",
    "\n",
    "  # Gradient computation\n",
    "  dw = gradient(X, Y, y_preds)\n",
    "\n",
    "  # Update weights\n",
    "  w -= learning_rate * dw\n",
    "\n",
    "  if epoch % 5 == 0:\n",
    "    print(\"Weights={}\".format(w), \"Loss={}\".format(l))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BbDrD0fIz7M7"
   },
   "source": [
    "## Linear Regression with Gradient Computation in PyTorch - Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e0cvZy9blDBJ",
    "outputId": "23f43d0f-ef89-48ba-f758-e559a7424322"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights=0.09999999403953552 Loss=30.0\n",
      "Weights=4.350164413452148 Loss=5.439698219299316\n",
      "Weights=4.913819789886475 Loss=5.007733345031738\n",
      "Weights=4.988570690155029 Loss=5.000136375427246\n",
      "Weights=4.998484134674072 Loss=5.000002861022949\n",
      "Weights=4.9997992515563965 Loss=5.0\n",
      "Weights=4.999971866607666 Loss=5.0\n",
      "Weights=4.999988555908203 Loss=5.0\n",
      "Weights=4.999988555908203 Loss=5.0\n",
      "Weights=4.999988555908203 Loss=5.0\n"
     ]
    }
   ],
   "source": [
    "# Training samples. This is the general training pipeline in PyTorch. Still a manual process\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "for epoch in range(1000):\n",
    "  y_pred = forward(x)\n",
    "\n",
    "  # Loss\n",
    "  l = loss(Y, y_pred)\n",
    "\n",
    "  # Gradients = Backward pass. Calculates dw and dJ/dw internally\n",
    "  l.backward()\n",
    "\n",
    "  # Update weights. Autogradient computation:\n",
    "  with torch.no_grad():\n",
    "    w -= learning_rate * w.grad\n",
    "\n",
    "  if epoch % 100==0:\n",
    "    print(\"Weights={}\".format(w), \"Loss={}\".format(l))\n",
    "\n",
    "  # Zero gradient\n",
    "  w.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZUDj_To3c2Z"
   },
   "source": [
    "## Loss Computation and PyTorch training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wKSCpedclC9u",
    "outputId": "46661196-b790-4c40-b7e9-8f3190a54c03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n",
      "Prediction before training: f(5)\" = 0.294\n",
      "Weights=0.0 Loss=30.821449279785156\n",
      "Weights=0.0 Loss=30.821449279785156\n",
      "Weights=0.0 Loss=30.821449279785156\n",
      "Weights=0.0 Loss=30.821449279785156\n",
      "Weights=0.0 Loss=30.821449279785156\n",
      "Prediction after training: f(5)\" = 0.294\n"
     ]
    }
   ],
   "source": [
    "# Design model (input, output size and forward pass)\n",
    "# Construct loss and optmizer\n",
    "# Design model (input, output size and forward pass)\n",
    "# Training loop - compute predictions, backward pass: gradients and update weights\n",
    "\n",
    "# Training samples. This is the general training pipeline in PyTorch\n",
    "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
    "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
    "\n",
    "X_test = torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "print(n_samples, n_features)\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "print(f'Prediction before training: f(5)\" = {model(X_test).item():.3f}')\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "\n",
    "for epoch in range(1000):\n",
    "\n",
    "  # Zero gradient. Setting dw, db and dZ to 0\n",
    "  optimizer.zero_grad()\n",
    "  y_pred = model(X)\n",
    "\n",
    "  # Loss\n",
    "  loss = nn.MSELoss()\n",
    "  optimizer = torch.optim.SGD([w], lr = learning_rate)\n",
    "\n",
    "  # Calculating loss\n",
    "  l = loss(Y, y_pred)\n",
    "\n",
    "  # Gradients = Backward pass. Calculates dw and dJ/dw internally\n",
    "  l.backward()\n",
    "\n",
    "  # Update weights. Autogradient computation:\n",
    "  optimizer.step()\n",
    "\n",
    "  if epoch % 200 == 0:\n",
    "    print(\"Weights={}\".format(w), \"Loss={}\".format(l)\n",
    "\n",
    "print(f'Prediction after training: f(5)\" = {model(X_test).item():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eghJ9t0pTEVy"
   },
   "source": [
    "## OOPs method - Preferred Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lJXDM3Y-_yNW",
    "outputId": "6f1f47a8-c807-44b2-aae4-913f0377a8e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n",
      "Prediction before training: f(5)\" = 1.320\n",
      "Weights=0.019999999552965164 Loss=19.759254455566406\n",
      "Weights=0.019999999552965164 Loss=19.759254455566406\n",
      "Prediction after training: f(5)\" = 9.285\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
    "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
    "\n",
    "X_test = torch.tensor([5], dtype=torch.float32)\n",
    "n_samples, n_features = X.shape\n",
    "print(n_samples, n_features)\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "model = nn.Linear(input_size,output_size)\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "\n",
    "  def __init__(self, input_dim, output_dim):\n",
    "\n",
    "    # Import all the parameters from the main class\n",
    "    super(LinearRegression, self).__init__()\n",
    "\n",
    "    # Define layers\n",
    "    self.lin = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.lin(x)\n",
    "\n",
    "model = LinearRegression(input_size, output_size)\n",
    "\n",
    "\n",
    "print(f'Prediction before training: f(5)\" = {model(X_test).item():.3f}')\n",
    "\n",
    "#Training\n",
    "learning_rate = 0.01\n",
    "\n",
    "w = torch.tensor(0.02, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "for epoch in range(100):\n",
    "  y_pred = model(X)\n",
    "\n",
    "  # Loss\n",
    "  loss = nn.MSELoss()\n",
    "  optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "  # Calculating loss\n",
    "  l = loss(Y, y_pred)\n",
    "\n",
    "  # Gradients = Backward pass. Calculates dw and dJ/dw internally. Autograd. Computes x.grad += dLoss/dx\n",
    "  l.backward()\n",
    "\n",
    "  # Update weights smoothened by softmax. Computes x += -lr * x.grad\n",
    "  optimizer.step()\n",
    "\n",
    "  if epoch % 200 == 0:\n",
    "    print(\"Weights={}\".format(w), \"Loss={}\".format(l))\n",
    "\n",
    "  # Zero gradient\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  if epoch % 200 == 0:\n",
    "    print(\"Weights={}\".format(w), \"Loss={}\".format(l))\n",
    "\n",
    "\n",
    "print(f'Prediction after training: f(5)\" = {model(X_test).item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "q2ACxk6N_yJ7",
    "outputId": "ef17486a-67bb-48e5-b807-a99c542e20e3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Design model. (input, output size, forward pass)\n",
    "# Construct loss and optimizer\n",
    "# Training loops\n",
    "  # forward pass: compute prediction and loss\n",
    "  # backward pass: gradients\n",
    "  # update weights\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare data\n",
    "# Adjust the linearity of the model with the help of noise parameter\n",
    "X_numpy, Y_numpy = datasets.make_regression(n_samples = 100, n_features = 1, noise = 5, random_state = 0)\n",
    "plt.scatter(X_numpy, Y_numpy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0DE6ACCZ_yHG",
    "outputId": "7a66e57b-d913-43bc-c931-be046139f438"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 29.0760\n",
      "Epoch [200/1000], Loss: 28.5431\n",
      "Epoch [300/1000], Loss: 28.5429\n",
      "Epoch [400/1000], Loss: 28.5429\n",
      "Epoch [500/1000], Loss: 28.5429\n",
      "Epoch [600/1000], Loss: 28.5429\n",
      "Epoch [700/1000], Loss: 28.5429\n",
      "Epoch [800/1000], Loss: 28.5429\n",
      "Epoch [900/1000], Loss: 28.5429\n",
      "Epoch [1000/1000], Loss: 28.5429\n"
     ]
    }
   ],
   "source": [
    "# Convert both the feature and label matrix to torch tensors. Preparing the data\n",
    "X = torch.from_numpy(X_numpy.astype(np.float32))\n",
    "Y = torch.from_numpy(Y_numpy.astype(np.float32))\n",
    "Y = Y.view(Y.shape[0], 1)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "# Defining the model\n",
    "input_size = n_features\n",
    "output_size = 1\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "# Loss and optimizer\n",
    "learning_rate = 0.02\n",
    "num_epochs = 1000\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "                            \n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "  # Convert numpy arrays to torch tensors\n",
    "  preds = model(X)\n",
    "  loss = criterion(preds, Y)\n",
    "\n",
    "  # Backward and optimize \n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  if (epoch+1) % 100 == 0:\n",
    "    print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "    # Think about the type of loss next time onwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "ZxBl8Vl2_yEZ",
    "outputId": "eda7b034-eff5-4624-bd08-b901cabd7464"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the graph\n",
    "predicted = model(torch.from_numpy(X_numpy.astype(np.float32))).detach().numpy()\n",
    "plt.plot(X, Y, 'ro', label='Original data')\n",
    "plt.plot(X, predicted, label='Fitted line')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbFJtrTHTtBb"
   },
   "source": [
    "# Custom Dataset - Tabular Form - without inheriting from PyTorch Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-P2v13wPTyuu"
   },
   "outputs": [],
   "source": [
    "class CustomDataset:\n",
    "  def __init__(self, data, targets):\n",
    "    self.data = data\n",
    "    self.targets = targets\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    current_sample = self.data[idx, :]\n",
    "    current_target = self.targets[idx]\n",
    "    return {\n",
    "        \"sample\": torch.tensor(current_sample), \n",
    "        \"target\": torch.tensor(current_target)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ewZCXioJTy5G"
   },
   "outputs": [],
   "source": [
    "# Load sklearn tool datasets\n",
    "from sklearn.datasets import make_classification \n",
    "data, targets = make_classification(n_samples=1000)\n",
    "\n",
    "# Pass the custom dataset and initialize it using the class defined above \n",
    "custom_data = CustomDataset(data, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DYuxtej0Ty1o",
    "outputId": "af09ab09-d474-44e2-b66d-34ad09e3186a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(custom_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "de_rAnmBTyzH",
    "outputId": "28c94a6c-b25b-410f-bd2d-1ad94c537b83"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sample': tensor([-0.3969,  1.1745, -0.1510, -1.1111,  1.3040, -1.2445, -0.3399,  0.8531,\n",
       "         -1.8495,  1.2958,  0.5172, -0.0913, -0.5608, -0.2307, -1.4907,  0.2580,\n",
       "         -0.3188,  0.4440, -0.9247,  0.1080], dtype=torch.float64),\n",
       " 'target': tensor(1)}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwpSlEEYi5cX"
   },
   "source": [
    "  # Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gmmqz391i5xl"
   },
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "input_size = 28 * 28    # 784\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='../../data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='../../data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    "# Data loader (input pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "# Logistic Regression Model \n",
    "model = nn.Linear(input_size, num_classes)\n",
    "\n",
    "\n",
    "# Loss and optimizer\n",
    "# nn.CrossEntropyLoss() computes softmax internally\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) \n",
    "\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "        # Reshape images to (batch_size, input_size)\n",
    "        images = images.reshape(-1, input_size)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "            \n",
    "# Testing the model. No gradient checking happpens for test data \n",
    "with torch.no_grad():\n",
    "  correct = 0\n",
    "  total = 0\n",
    "\n",
    "  for images, labels in test_loader:\n",
    "    images = images.reshape(-1, input_size)\n",
    "    outputs = model(images)\n",
    "    print(outputs.data)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "    print(outputs.data)\n",
    "\n",
    "    # Maybe labels are the indices themselves\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()  \n",
    "\n",
    "print('Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "piZKTYFFmtoI"
   },
   "outputs": [],
   "source": [
    "# Testing the model. No gradient checking happpens for test data \n",
    "with torch.no_grad():\n",
    "  correct = 0\n",
    "  total = 0\n",
    "\n",
    "  for images, labels in test_loader:\n",
    "    images = images.reshape(-1, input_size)\n",
    "    outputs = model(images)\n",
    "    print(outputs.data)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "    print(outputs.data)\n",
    "\n",
    "    # Maybe labels are the indices themselves\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()  \n",
    "\n",
    "print('Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TO2lDZl82yo6"
   },
   "source": [
    "# Debugging a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tP4HXPFj25c-"
   },
   "source": [
    "Checklist\n",
    "<br>\n",
    "<input type=\"checkbox\"> Building a simpler  model-check for hyperparams and dims\n",
    "  \n",
    "\n",
    "<input type=\"checkbox\"> Training model on a single or 2-5 data points\n",
    "<p>\n",
    "This checks for training set overfitting on those 2-3 points and a validation accuracy as good as random guessing. Try for a simple epochs before progressing\n",
    "</p>\n",
    "\n",
    "<input type=\"checkbox\"> Confirming the loss function \n",
    "<p>\n",
    "The loss is appropriate for task - using categorical cross-entropy for multi-class classification problems or using focal loss to address class imbalance. The initial loss shouldn't be much bigger or smaller. Claculate a base minimum probability yourself \n",
    "</p>\n",
    "\n",
    "<input type=\"checkbox\"> Check intermediate outputs and connections  \n",
    "<p>\n",
    "The loss is appropriate for task - using categorical cross-entropy for multi-class classification problems or using focal loss to address class imbalance. The initial loss shouldn't be much bigger or smaller. Claculate a base minimum probability yourself \n",
    "</p>\n",
    "\n",
    "<input type=\"checkbox\"> Diagnose parameters \n",
    "<p>\n",
    "From SGD to learning rates, identifying the right combination - or figuring out the wrong ones\n",
    "</p>\n",
    "\n",
    "<input type=\"checkbox\">  Diagnose parameters \n",
    "<p>\n",
    "From SGD to learning rates, identifying the right combination - or figuring out the wrong ones\n",
    "</p>\n",
    "\n",
    "<input type=\"checkbox\"> Tracking your work \n",
    "<p>\n",
    "As a baseline, track your experimentation and key modelling facts \n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIkQeE_g_nlj"
   },
   "source": [
    "Requirements:\n",
    "<p>\n",
    "<input type = \"checkbox\"> Build ANN - Simple one - perform the debugging features\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<input type = \"checkbox\"> Hyperparameter tuning - Play with 3 different values\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<input type = \"checkbox\"> Loss Function - Check for 3 different values\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<input type = \"checkbox\"> Play with different kinds of values \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjeIoaNiEe1T"
   },
   "source": [
    "##Recurrent Neural Nets - Vanilla RNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "491kIlSxNzPb",
    "outputId": "0c5f2ec6-61cb-46fd-8402-b59a3efe4ce4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../../data/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d143d09727ff4577a2c91990b28896be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/MNIST\\raw\\train-images-idx3-ubyte.gz to ../../data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../../data/MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baea9855210e4dea9456ccae43ecc18c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/MNIST\\raw\\train-labels-idx1-ubyte.gz to ../../data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../../data/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d50f613b27c447baca73f5e4887a8c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/MNIST\\raw\\t10k-images-idx3-ubyte.gz to ../../data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../../data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "900e32758a374641a01a7e597c940581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ../../data/MNIST\\raw\n",
      "\n",
      "Accuracy of the model on the 10000 test images: 97.16999816894531 %\n"
     ]
    }
   ],
   "source": [
    "# Defining hyperparameters for the Recurrent Network. Switch to GPU runtime for faster processing \n",
    "sequence_length = 28\n",
    "input_dim = 28\n",
    "hidden_dim = 128\n",
    "layers_dim = 2\n",
    "output_dim = 10\n",
    "batch_size = 100\n",
    "num_epochs = 20\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Training on MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root = '../../data/',\n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    "# Data Loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, \n",
    "                                           batch_size = batch_size, \n",
    "                                           shuffle = True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset, \n",
    "                                          batch_size = batch_size, \n",
    "                                          shuffle = False)\n",
    "\n",
    "\n",
    "# When the Network class is instantiated (Ex: net = Network()), \n",
    "# all the statements within __init__ are executed (the constructor). \n",
    "# Later, when you run your network on some batch of data, you write output = net(x), which invokes the __call__ method. \n",
    "# Your Network class simply inherits the __call__ method of the nn.Module class. \n",
    "# It is here where the forward method is called\n",
    "# Learn about different optimizers and experiment with them \n",
    "\n",
    "\n",
    "# Creating an RNN model PyTorch \n",
    "class RNNModel(nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim, layers_dim, output_dim):\n",
    "    super(RNNModel, self).__init__()\n",
    "\n",
    "    # Initialize Input Size \n",
    "    self.input_dim = input_dim\n",
    "\n",
    "    # Initialize Number of Hidden Layers\n",
    "    self.hidden_dim = hidden_dim\n",
    "\n",
    "    # Initialize the no. of RNN layers\n",
    "    self.layers_dim = layers_dim\n",
    "\n",
    "    # Initialize output dimension\n",
    "    self.output_dim = output_dim\n",
    "\n",
    "    # RNN\n",
    "    self.rnn = nn.RNN(self.input_dim, self.hidden_dim, self.layers_dim, batch_first=True, nonlinearity='relu')\n",
    "\n",
    "    # Readout Layer\n",
    "    self.fc = nn.Linear(self.hidden_dim, self.output_dim) \n",
    "\n",
    "  def forward(self, x):\n",
    "\n",
    "    # Initializing the first hidden state with zeroes\n",
    "    # (layer_dim, batch_size, hidden_dim)\n",
    "    # No need of explicitly initializing the initial hidden state to zeroes. Its implicitly understood.\n",
    "    self.h0 = torch.zeros(self.layers_dim, x.size(0), self.hidden_dim) \n",
    "\n",
    "    # We need to detach the hidden state to prevent exploding/vanishing gradients \n",
    "    # This is part of truncated backpropagation through time(BPTT)\n",
    "    out, hn = self.rnn(x, self.h0.detach())\n",
    "\n",
    "    # Index hidden state of last timestep \n",
    "    # Index hidden state of last time-step\n",
    "    # out.size() --> 100, 28, 10 (batch_size, input_dim, out_dim)\n",
    "    # out[:, -1, :] --> 100, 10 --> just want last time step hidden states!\n",
    "    out = self.fc(out[:, -1, :])\n",
    "    # new out size() ---> 100, 10\n",
    "    return out \n",
    "\n",
    "# Instantiate model class and assign it to object\n",
    "model = RNNModel(input_dim, hidden_dim, layers_dim, output_dim)\n",
    "\n",
    "# Instatiate classification loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Instantiate Optimizer Class. Refer documentation\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Train the model \n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "  for i, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "    # Load images as tensors with gradient accumulation abilities. The '.requires_grad()_' is redundant.\n",
    "    images = images.reshape(-1, sequence_length, input_dim).requires_grad_()\n",
    "\n",
    "    # Clear gradients WRT parameters\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass to get outputs/logits \n",
    "    # outputs.size() --> 100, 10\n",
    "    outputs = model(images)\n",
    "\n",
    "    # Calculate Loss: softmax --> cross entropy loss\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    # Getting gradients WRT parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Updating parameters \n",
    "    optimizer.step()\n",
    "\n",
    "    iter += 1\n",
    "\n",
    "    # Writing the below command is optional \n",
    "    # model.eval()\n",
    "\n",
    "\n",
    "# Test the model. No gradient checking happpens occurs for test data \n",
    "with torch.no_grad():\n",
    "  \n",
    "  # Calculate accuracy \n",
    "  correct = 0\n",
    "  total = 0\n",
    "\n",
    "  # Iterate through test dataset \n",
    "  for images, labels in test_loader:\n",
    "\n",
    "    # Load images to a torch tensor with gradient accumulation abilities \n",
    "    # Sequence length refers to the no. of previous images that will be studied. But that won't make much for images as there is no contextual\n",
    "    # information of one image WRT another \n",
    "    images = images.reshape(-1, sequence_length, input_dim)\n",
    "\n",
    "    # Forward pass to get logits/output \n",
    "    outputs = model(images)\n",
    "\n",
    "    # Get predictions from the maximum value \n",
    "    _, preds = torch.max(outputs.data, 1)\n",
    "\n",
    "    # Total number of labels in the batch \n",
    "    total += labels.size(0)\n",
    "\n",
    "    # Total correct predictions \n",
    "    correct += (preds == labels).sum()\n",
    "\n",
    "  # Calculate accuracy of model \n",
    "  accuracy = (100 * correct)/total \n",
    "\n",
    "  # Print Loss\n",
    "  print('Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L2pfAF_dEif-"
   },
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "sequence_length = 28\n",
    "input_size = 28\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "num_epochs = 2\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "# Training on MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root = '../../data/',\n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    "# Data Loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, \n",
    "                                           batch_size = batch_size, \n",
    "                                           shuffle = True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset, \n",
    "                                          batch_size = batch_size, \n",
    "                                          shuffle = False)\n",
    "\n",
    "# Recurrent Neural Network (many-to-one)\n",
    "class RNN(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "    super(RNN, self).__init__()\n",
    "\n",
    "    # Implementing a stacked LSTM-RNN module - (28, 100, 2)\n",
    "    self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "    #The weight-matrix is transposed during computation\n",
    "    self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "  def forward(self, x):\n",
    "\n",
    "    # Set initial hidden and cell states - (2, 100, 128)\n",
    "    h0 = torch.zeros(num_layers, x.size(0), hidden_size).to(device)\n",
    "    c0 = torch.zeros(num_layers, x.size(0), hidden_size).to(device)\n",
    "\n",
    "    # Forward propagate LSTM\n",
    "    out, _ = self.lstm(x, (h0, c0)) # out: tensor of shape(batch_size, seq_length, hidden_size)\n",
    "\n",
    "    # Decode the hidden state of the last time step\n",
    "    out = self.fc(out[:, -1, :])\n",
    "    return out\n",
    "\n",
    "    # Decode the hidden state of the last time step\n",
    "    out = self.fc(out[:, -1, :])\n",
    "    return out\n",
    "\n",
    "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "  for i, (images, labels) in enumerate(train_loader):\n",
    "    images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(images)\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "    if (i+1) % 100 == 0:\n",
    "      print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "\n",
    "# Test the model. No gradient checking happpens occurs for test data \n",
    "with torch.no_grad():\n",
    "  \n",
    "  # Calculate accuracy \n",
    "  correct = 0\n",
    "  total = 0\n",
    "\n",
    "  # Iterate through test dataset \n",
    "  for images, labels in test_loader:\n",
    "\n",
    "    # Load images to a torch tensor with gradient accumulation abilities \n",
    "    # Sequence length refers to the no. of previous images that will be studied. But that won't make much for images as there is no contextual\n",
    "    # information of one image WRT another \n",
    "    images = images.reshape(-1, sequence_length, input_dim)\n",
    "\n",
    "    # Forward pass to get logits/output \n",
    "    outputs = model(images)\n",
    "\n",
    "    # Get predictions from the maximum value \n",
    "    _, preds = torch.max(outputs.data, 1)\n",
    "\n",
    "    # Total number of labels in the batch \n",
    "    total += labels.size(0)\n",
    "\n",
    "    # Total correct predictions \n",
    "    correct += (preds == labels).sum()\n",
    "\n",
    "# Calculate accuracy of model \n",
    "accuracy = (100 * correct)/total \n",
    "\n",
    "# Print Loss\n",
    "print('Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uHXbKxNvEjKm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gCUrpxs2EjHE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I8uabf9xEjDZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6JBrmo00Ei-E"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ZoLd6PbEi6v"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Pytorch-final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
