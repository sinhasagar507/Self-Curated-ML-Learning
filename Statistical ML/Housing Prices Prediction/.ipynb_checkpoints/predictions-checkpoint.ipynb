{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T12:24:10.434264Z",
     "iopub.status.busy": "2022-06-14T12:24:10.433371Z",
     "iopub.status.idle": "2022-06-14T12:24:12.836653Z",
     "shell.execute_reply": "2022-06-14T12:24:12.835781Z",
     "shell.execute_reply.started": "2022-06-14T12:24:10.434120Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import sklearn\n",
    "\n",
    "# Feature Engineering and Feature Transformation modules\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures, RobustScaler \n",
    "\n",
    "# Feature Transformation modules \n",
    "from scipy.stats import skew, boxcox_normmax\n",
    "from scipy.special import boxcox1p\n",
    "\n",
    "# Feature Selection and model selection modules \n",
    "from sklearn.feature_selection import RFE, VarianceThreshold, f_regression, SelectFromModel \n",
    "from sklearn.ensemble import ExtraTreesRegressor \n",
    "from sklearn.feature_selection import f_regression, SelectFromModel\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, RandomizedSearchCV, StratifiedKFold, cross_val_score\n",
    "\n",
    "\n",
    "# Module of ML models \n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, LogisticRegression, LassoCV, RidgeCV, ElasticNetCV \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC, SVC, SVR \n",
    "from xgboost import XGBRFRegressor, XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from mlxtend.regressor import StackingCVRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, AdaBoostRegressor, BaggingRegressor, ExtraTreesRegressor \n",
    "\n",
    "\n",
    "# Layout Design \n",
    "sns.set_context(\"paper\", font_scale = 1, rc={\"grid.linewidth\": 3})\n",
    "pd.set_option('display.max_rows', 100, 'display.max_columns', 400)\n",
    "\n",
    "# Evaluation Metric modules\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, mean_absolute_error, r2_score\n",
    "\n",
    "# Pipeline modules \n",
    "# First Inherit the base classes\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.compose import make_column_transformer, ColumnTransformer \n",
    "from sklearn.pipeline import make_union, FeatureUnion\n",
    "import pickle\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T12:24:23.905106Z",
     "iopub.status.busy": "2022-06-14T12:24:23.904691Z",
     "iopub.status.idle": "2022-06-14T12:24:23.910974Z",
     "shell.execute_reply": "2022-06-14T12:24:23.909846Z",
     "shell.execute_reply.started": "2022-06-14T12:24:23.905066Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initializing random generators for models \n",
    "# Set a seed value\n",
    "seed_value= 12321 \n",
    "\n",
    "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set `tensorflow` pseudo-random generator at a fixed value\n",
    "# import tensorflow as tf\n",
    "# tf.set_random_seed(seed_value)\n",
    "\n",
    "# 5. For layers that introduce randomness like dropout, make sure to set seed values \n",
    "# model.add(Dropout(0.25, seed=seed_value))\n",
    "\n",
    "# 6. Configure a new global `tensorflow` session\n",
    "# from keras import backend as K\n",
    "# session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "# sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "# K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRc9vklW1_3w"
   },
   "source": [
    "# Elementary Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T12:24:28.145906Z",
     "iopub.status.busy": "2022-06-14T12:24:28.145161Z",
     "iopub.status.idle": "2022-06-14T12:24:28.340584Z",
     "shell.execute_reply": "2022-06-14T12:24:28.339597Z",
     "shell.execute_reply.started": "2022-06-14T12:24:28.145851Z"
    },
    "id": "lido9EaHKm2L",
    "outputId": "623eafb8-e70a-4375-fb08-442d7170fff9"
   },
   "outputs": [],
   "source": [
    "# Reading the dataset\n",
    "df_train = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\n",
    "df_test = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\n",
    "df_origin = pd.read_csv('../input/additional-dataset-for-training/AmesHousing.csv')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T12:24:30.968906Z",
     "iopub.status.busy": "2022-06-14T12:24:30.968097Z",
     "iopub.status.idle": "2022-06-14T12:24:30.976617Z",
     "shell.execute_reply": "2022-06-14T12:24:30.975677Z",
     "shell.execute_reply.started": "2022-06-14T12:24:30.968843Z"
    }
   },
   "outputs": [],
   "source": [
    "# Identify if there exists an extra column in the original data which isn't present in provided train data\n",
    "print(len(df_origin))\n",
    "print()\n",
    "extra_cols = [col for col in df_origin.columns if col not in df_train.columns]\n",
    "print(extra_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "1. We can observe that the column names are different in original data as to provided train data\n",
    "2. There is an extra feature named 'PID' - which is parcel identification number - can be used with city website for parcel review. Since its an Id, we will drop it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T12:24:34.044070Z",
     "iopub.status.busy": "2022-06-14T12:24:34.043410Z",
     "iopub.status.idle": "2022-06-14T12:24:34.050962Z",
     "shell.execute_reply": "2022-06-14T12:24:34.050068Z",
     "shell.execute_reply.started": "2022-06-14T12:24:34.044029Z"
    }
   },
   "outputs": [],
   "source": [
    "# Droppping the 'PID' feature \n",
    "df_origin.drop(['PID'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T05:02:04.294682Z",
     "iopub.status.busy": "2022-06-14T05:02:04.294285Z",
     "iopub.status.idle": "2022-06-14T05:02:04.346911Z",
     "shell.execute_reply": "2022-06-14T05:02:04.345582Z",
     "shell.execute_reply.started": "2022-06-14T05:02:04.294649Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use the origin data as new training data\n",
    "# df_train.columns = df_origin.columns\n",
    "df_combined_train = df_origin\n",
    "df_combined_train.drop_duplicates(inplace=True)\n",
    "df_combined_train.columns = df_combined_train.columns.str.replace(' ', '')\n",
    "print(df_combined_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T05:02:18.269879Z",
     "iopub.status.busy": "2022-06-14T05:02:18.269291Z",
     "iopub.status.idle": "2022-06-14T05:02:18.345374Z",
     "shell.execute_reply": "2022-06-14T05:02:18.344129Z",
     "shell.execute_reply.started": "2022-06-14T05:02:18.269841Z"
    }
   },
   "outputs": [],
   "source": [
    "# Inspecting the train data\n",
    "df_combined_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T05:02:20.491434Z",
     "iopub.status.busy": "2022-06-14T05:02:20.491022Z",
     "iopub.status.idle": "2022-06-14T05:02:20.567477Z",
     "shell.execute_reply": "2022-06-14T05:02:20.566327Z",
     "shell.execute_reply.started": "2022-06-14T05:02:20.491399Z"
    }
   },
   "outputs": [],
   "source": [
    "# Inspecting the test dataset\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T05:02:23.365883Z",
     "iopub.status.busy": "2022-06-14T05:02:23.365418Z",
     "iopub.status.idle": "2022-06-14T05:02:23.420007Z",
     "shell.execute_reply": "2022-06-14T05:02:23.418461Z",
     "shell.execute_reply.started": "2022-06-14T05:02:23.365843Z"
    },
    "id": "lAQovHQ1L0qJ",
    "outputId": "dd5f6c98-a0f0-449d-bc67-d7a657ce2289"
   },
   "outputs": [],
   "source": [
    "# Gathering Appropriate Information \n",
    "# Drop 'Order' column from training data and 'Id' column from test data\n",
    "df_combined_train.drop(['Order'], axis=1, inplace=True)\n",
    "df_test.drop(['Id'], axis=1, inplace=True)\n",
    "print(df_combined_train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T05:02:27.077044Z",
     "iopub.status.busy": "2022-06-14T05:02:27.076548Z",
     "iopub.status.idle": "2022-06-14T05:02:27.090213Z",
     "shell.execute_reply": "2022-06-14T05:02:27.088720Z",
     "shell.execute_reply.started": "2022-06-14T05:02:27.077002Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove duplicate values and change the wrong data types \n",
    "df_combined_train = df_combined_train.loc[~df_combined_train.index.duplicated(), :]\n",
    "df_test = df_test.loc[~df_test.index.duplicated(), :]\n",
    "\n",
    "# Hence, there is no column with dubious or incorrect data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T05:02:30.864208Z",
     "iopub.status.busy": "2022-06-14T05:02:30.863574Z",
     "iopub.status.idle": "2022-06-14T05:02:30.873095Z",
     "shell.execute_reply": "2022-06-14T05:02:30.871420Z",
     "shell.execute_reply.started": "2022-06-14T05:02:30.864147Z"
    },
    "id": "iOjkq3btL7Wm",
    "outputId": "f44fb7e0-1af3-4d2c-b5b2-efbada48b118"
   },
   "outputs": [],
   "source": [
    "print(df_combined_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T05:02:32.718038Z",
     "iopub.status.busy": "2022-06-14T05:02:32.717073Z",
     "iopub.status.idle": "2022-06-14T05:02:32.852352Z",
     "shell.execute_reply": "2022-06-14T05:02:32.851044Z",
     "shell.execute_reply.started": "2022-06-14T05:02:32.717975Z"
    },
    "id": "oQld1hwQMGRa",
    "outputId": "e5a88aad-0d1d-4fb4-fcb6-7f31cb5c6353"
   },
   "outputs": [],
   "source": [
    "# Getting insights of the features and outliers\n",
    "df_combined_train.describe([0.25,0.50,0.75,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T05:02:35.340332Z",
     "iopub.status.busy": "2022-06-14T05:02:35.339363Z",
     "iopub.status.idle": "2022-06-14T05:02:35.351371Z",
     "shell.execute_reply": "2022-06-14T05:02:35.349606Z",
     "shell.execute_reply.started": "2022-06-14T05:02:35.340280Z"
    }
   },
   "outputs": [],
   "source": [
    "# Separate numerical and categorical columns\n",
    "numerical_cols = [col for col in df_combined_train.columns if df_combined_train[col].dtype!='object']\n",
    "categorical_cols = [col for col in df_combined_train.columns if df_combined_train[col].dtype=='object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T05:02:36.988474Z",
     "iopub.status.busy": "2022-06-14T05:02:36.988038Z",
     "iopub.status.idle": "2022-06-14T05:02:37.053271Z",
     "shell.execute_reply": "2022-06-14T05:02:37.052049Z",
     "shell.execute_reply.started": "2022-06-14T05:02:36.988439Z"
    },
    "id": "pNpQjQ720GGJ",
    "outputId": "84f5b906-e925-4aa4-f08e-3a2ff7a16408"
   },
   "outputs": [],
   "source": [
    "# Checking percentage of null values present in training dataset \n",
    "missing_num = df_combined_train.isna().sum().sort_values(ascending=False)\n",
    "missing_perc = (df_combined_train.isna().sum()/len(df_combined_train)*100).sort_values(ascending=False)\n",
    "missing_perc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZSZwp70nudt"
   },
   "source": [
    "EDA only for learning purposes done separately. Else you should always combine both the train and test sets and then only perform EDA for easier analysis. That is done later anyways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T05:02:40.072736Z",
     "iopub.status.busy": "2022-06-14T05:02:40.072363Z",
     "iopub.status.idle": "2022-06-14T05:02:40.126425Z",
     "shell.execute_reply": "2022-06-14T05:02:40.125602Z",
     "shell.execute_reply.started": "2022-06-14T05:02:40.072701Z"
    },
    "id": "PNYPC75Iy_8O"
   },
   "outputs": [],
   "source": [
    "# Calculating percentage of null values\n",
    "def null_values(dataframe):\n",
    "  missing_values = dataframe.isna().sum().sort_values(ascending=False)\n",
    "  missing_perc = (((dataframe.isna().sum())/len(dataframe))*100).sort_values(ascending=False)\n",
    "  return missing_values, missing_perc\n",
    "\n",
    "#Passing in the training and test datasets to calculate the percentage of missing values in training and test data\n",
    "null_sum_train, null_perc_train = null_values(df_combined_train)\n",
    "null_sum_test, null_perc_test = null_values(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T05:02:43.095837Z",
     "iopub.status.busy": "2022-06-14T05:02:43.095332Z",
     "iopub.status.idle": "2022-06-14T05:02:43.104306Z",
     "shell.execute_reply": "2022-06-14T05:02:43.102858Z",
     "shell.execute_reply.started": "2022-06-14T05:02:43.095805Z"
    },
    "id": "dZF5qFtT1S6O",
    "outputId": "86d3b700-b847-4989-fb4a-4af54b2b3064"
   },
   "outputs": [],
   "source": [
    "null_sum_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T05:02:46.113467Z",
     "iopub.status.busy": "2022-06-14T05:02:46.112741Z",
     "iopub.status.idle": "2022-06-14T05:02:46.123272Z",
     "shell.execute_reply": "2022-06-14T05:02:46.122064Z",
     "shell.execute_reply.started": "2022-06-14T05:02:46.113428Z"
    },
    "id": "jH03PveD3p9f",
    "outputId": "60abcc53-c3b8-45ac-b781-f3f53088430a"
   },
   "outputs": [],
   "source": [
    "null_perc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T05:02:48.708243Z",
     "iopub.status.busy": "2022-06-14T05:02:48.707863Z",
     "iopub.status.idle": "2022-06-14T05:02:48.718027Z",
     "shell.execute_reply": "2022-06-14T05:02:48.716961Z",
     "shell.execute_reply.started": "2022-06-14T05:02:48.708212Z"
    },
    "id": "9M6ABUiZnBhr"
   },
   "outputs": [],
   "source": [
    "miss_train_sum_perc = pd.concat([null_sum_train, null_perc_train], axis=1, keys=['Sum', 'Percentage'])\n",
    "miss_test_sum_perc = pd.concat([null_sum_test, null_perc_test], axis=1, keys=['Sum', 'Percentage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T05:02:50.971417Z",
     "iopub.status.busy": "2022-06-14T05:02:50.970748Z",
     "iopub.status.idle": "2022-06-14T05:02:50.989885Z",
     "shell.execute_reply": "2022-06-14T05:02:50.988752Z",
     "shell.execute_reply.started": "2022-06-14T05:02:50.971360Z"
    },
    "id": "LAsPzF-zovzL",
    "outputId": "d36fb0c6-647a-4ead-8828-284ebf08ca2c"
   },
   "outputs": [],
   "source": [
    "miss_train_plot = miss_train_sum_perc[miss_train_sum_perc['Percentage']>0]\n",
    "miss_train_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T05:02:54.937068Z",
     "iopub.status.busy": "2022-06-14T05:02:54.936601Z",
     "iopub.status.idle": "2022-06-14T05:02:54.954643Z",
     "shell.execute_reply": "2022-06-14T05:02:54.952989Z",
     "shell.execute_reply.started": "2022-06-14T05:02:54.937017Z"
    }
   },
   "outputs": [],
   "source": [
    "miss_test_plot = miss_test_sum_perc[miss_test_sum_perc['Percentage']>0]\n",
    "miss_test_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8ZxsE327W0g"
   },
   "source": [
    "**19 attributes have missing values and 5 features( PoolQC,MiscFeature,Alley,Fence,FireplaceQu) have missing percentage greater than 45%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T05:02:57.651489Z",
     "iopub.status.busy": "2022-06-14T05:02:57.650819Z",
     "iopub.status.idle": "2022-06-14T05:02:57.693724Z",
     "shell.execute_reply": "2022-06-14T05:02:57.691735Z",
     "shell.execute_reply.started": "2022-06-14T05:02:57.651429Z"
    },
    "id": "vANqlZ4g6kjY",
    "outputId": "dd7c652b-bf7d-480a-f2c7-92acfb56e737"
   },
   "outputs": [],
   "source": [
    "# Printing the numerical dataframe\n",
    "df_numerical_train = df_combined_train.select_dtypes(include=['int64','float64'])\n",
    "df_numerical_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T05:02:59.717363Z",
     "iopub.status.busy": "2022-06-14T05:02:59.716823Z",
     "iopub.status.idle": "2022-06-14T05:02:59.772317Z",
     "shell.execute_reply": "2022-06-14T05:02:59.771325Z",
     "shell.execute_reply.started": "2022-06-14T05:02:59.717321Z"
    },
    "id": "pK2bfg1Sh8Vw",
    "outputId": "f8e2d1eb-7dd0-486b-ac37-b5c79278d4f7"
   },
   "outputs": [],
   "source": [
    "# Printing the categorical dataframe\n",
    "df_categorical_train = df_combined_train.select_dtypes(exclude=['int64','float64'])\n",
    "df_categorical_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T05:03:10.868060Z",
     "iopub.status.busy": "2022-06-14T05:03:10.867460Z",
     "iopub.status.idle": "2022-06-14T05:03:10.898895Z",
     "shell.execute_reply": "2022-06-14T05:03:10.897838Z",
     "shell.execute_reply.started": "2022-06-14T05:03:10.868004Z"
    }
   },
   "outputs": [],
   "source": [
    "# No.of unique values in each of the numerical columns\n",
    "for col in df_numerical_train.columns:\n",
    "    print(str(col)+\"-\"*len(str(col))+str(df_numerical_train[col].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T05:03:13.455998Z",
     "iopub.status.busy": "2022-06-14T05:03:13.455235Z",
     "iopub.status.idle": "2022-06-14T05:03:13.516005Z",
     "shell.execute_reply": "2022-06-14T05:03:13.514680Z",
     "shell.execute_reply.started": "2022-06-14T05:03:13.455942Z"
    }
   },
   "outputs": [],
   "source": [
    "# No.of unique values in each of the categorical columns\n",
    "for col in df_categorical_train.columns:\n",
    "    print(str(col)+\"-\"*len(str(col))+str(df_categorical_train[col].nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations:\n",
    "1. There are 80 columns in this dataset. Need to do a bit of feature-selection later on \n",
    "2. There are several columns(numerical+categorical) that have few no. of unique variables when compared to the overall count of all training instances\n",
    "3. The test dataset size is almost equivalent to the size of training dataset. Do I have to append more training instance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NrLgfxz2-6SO"
   },
   "source": [
    "# Data Cleaning + Visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T05:03:20.753074Z",
     "iopub.status.busy": "2022-06-14T05:03:20.752670Z",
     "iopub.status.idle": "2022-06-14T05:03:20.760558Z",
     "shell.execute_reply": "2022-06-14T05:03:20.759192Z",
     "shell.execute_reply.started": "2022-06-14T05:03:20.753037Z"
    },
    "id": "LHyyT8Zy-4ml"
   },
   "outputs": [],
   "source": [
    "# Important \n",
    "def showvalues(ax,m=None):\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(\"%.1f\" % p.get_height(), (p.get_x() + p.get_width() / 2., p.get_height()),\\\n",
    "                    ha='center', va='center', fontsize=14, color='k', rotation=0, xytext=(0, 7),\\\n",
    "                    textcoords='offset points',fontweight='light',alpha=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T05:03:22.955179Z",
     "iopub.status.busy": "2022-06-14T05:03:22.954726Z",
     "iopub.status.idle": "2022-06-14T05:03:24.436098Z",
     "shell.execute_reply": "2022-06-14T05:03:24.435022Z",
     "shell.execute_reply.started": "2022-06-14T05:03:22.955141Z"
    },
    "id": "Lil3vzpb_IsE",
    "outputId": "59b12f31-0485-4631-edda-a6a528b5bd03"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "plt.subplot(2, 1, 1)\n",
    "ax1 = sns.barplot(x=miss_train_plot.index, y='Percentage', data=miss_train_plot)\n",
    "showvalues(ax1)\n",
    "ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "ax2 = sns.barplot(x=miss_test_plot.index, y='Percentage', data=miss_test_plot)\n",
    "showvalues(ax2)\n",
    "ax2.set_xticklabels(ax2.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T05:03:29.540682Z",
     "iopub.status.busy": "2022-06-14T05:03:29.540076Z",
     "iopub.status.idle": "2022-06-14T05:03:29.551111Z",
     "shell.execute_reply": "2022-06-14T05:03:29.550210Z",
     "shell.execute_reply.started": "2022-06-14T05:03:29.540644Z"
    },
    "id": "MflcZu6JvT-z",
    "outputId": "ee8f1256-b69c-4a39-9e5d-8771e8311d6b"
   },
   "outputs": [],
   "source": [
    "len(df_combined_train.select_dtypes(include=['int64','float64']).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T05:11:39.770004Z",
     "iopub.status.busy": "2022-06-14T05:11:39.769427Z",
     "iopub.status.idle": "2022-06-14T05:11:39.778199Z",
     "shell.execute_reply": "2022-06-14T05:11:39.777223Z",
     "shell.execute_reply.started": "2022-06-14T05:11:39.769954Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T05:03:31.574008Z",
     "iopub.status.busy": "2022-06-14T05:03:31.573576Z",
     "iopub.status.idle": "2022-06-14T05:03:46.339604Z",
     "shell.execute_reply": "2022-06-14T05:03:46.338583Z",
     "shell.execute_reply.started": "2022-06-14T05:03:31.573962Z"
    },
    "id": "6T_y7OMjxeCJ",
    "outputId": "a94d3558-75c0-4b6c-fc43-5fba014c5d16"
   },
   "outputs": [],
   "source": [
    "# Visualising numerical predictor variables with Target Variables, and this as well can be used for univariate distributions as well\n",
    "# df_num_train = df_numerical_train[[col for col in df_numerical_train.columns if col != 'MS SubClass']]\n",
    "fig,axs= plt.subplots(12,3,figsize=(20, 80))\n",
    "\n",
    "# adjust horizontal space between plots \n",
    "fig.subplots_adjust(hspace=0.6)\n",
    "\n",
    "# We need to flatten the axes for iterating over them. Here the axes in the dimension [12, 3] is transformed to a vector consisting of 12*3 = 36 values.\n",
    "for i,ax in zip(df_numerical_train.columns, axs.flatten()):\n",
    "    sns.scatterplot(x=i, y='SalePrice', hue='SalePrice',data=df_numerical_train, ax=ax, palette='viridis_r')\n",
    "    plt.xlabel(i,fontsize=12)\n",
    "    plt.ylabel('SalePrice',fontsize=12)\n",
    "\n",
    "    # ax.set_yticks(np.arange(0,900001,100000))\n",
    "    ax.set_title('SalePrice'+' - '+str(i),fontweight='bold',size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "1. We can observe that the house prices rise steadily with increase in rating quality \n",
    "2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Justifications for not using KNN**\n",
    "1. KNN algorithm works best for classification where there is a clear demarcation of feature values and corresponding labels as clusters\n",
    "2. The number of unique target values is large enough and non-existence of a large number of training samples make it too difficult for the algorithm to segregate target lables and interpolate the same on test data\n",
    "\n",
    "**Justification for not using SVR and Kernel SVR**\n",
    "1. SVR is best utilized for estimating values where the  no. of dimensions >= no. of training samples. In short, it is favoured for smaller datasets\n",
    "2. The decision boundary can't be estimated easily due to the lack of a particular progression pattern between the feature and the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T05:43:13.270632Z",
     "iopub.status.busy": "2022-06-14T05:43:13.270019Z",
     "iopub.status.idle": "2022-06-14T05:43:27.222341Z",
     "shell.execute_reply": "2022-06-14T05:43:27.221264Z",
     "shell.execute_reply.started": "2022-06-14T05:43:13.270584Z"
    },
    "id": "XNCC0SJe5wE-",
    "outputId": "92d909ac-b57e-4dc2-e5d5-bc9f12873353"
   },
   "outputs": [],
   "source": [
    "# Visualizing categorical predictors with target variable\n",
    "def facetgrid_boxplot(x, y, **kwargs):\n",
    "  sns.boxplot(x=x, y=y)\n",
    "  x = plt.xticks(rotation=90)\n",
    "\n",
    "# pd.melt is a useful function. You have written its functionality in your notebook.\n",
    "f = pd.melt(df_combined_train, id_vars=['SalePrice'], value_vars=sorted(df_categorical_train.columns))\n",
    "g = sns.FacetGrid(f, col='variable', col_wrap=3, sharex=False, sharey=False)\n",
    "\n",
    "# Mapping onto the function where it will plot the boxplot\n",
    "g = g.map(facetgrid_boxplot, 'value', 'SalePrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T05:43:36.628126Z",
     "iopub.status.busy": "2022-06-14T05:43:36.627606Z",
     "iopub.status.idle": "2022-06-14T05:43:36.644684Z",
     "shell.execute_reply": "2022-06-14T05:43:36.643317Z",
     "shell.execute_reply.started": "2022-06-14T05:43:36.628067Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the distribution of dataframe 'f'\n",
    "f.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OuraXcwsTGt-"
   },
   "source": [
    "***SalePrice isn't normally distributed. It has a right skewed distribution.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T05:48:56.444400Z",
     "iopub.status.busy": "2022-06-14T05:48:56.443889Z",
     "iopub.status.idle": "2022-06-14T05:48:56.798371Z",
     "shell.execute_reply": "2022-06-14T05:48:56.796366Z",
     "shell.execute_reply.started": "2022-06-14T05:48:56.444354Z"
    },
    "id": "KYZx6JJqSh8O",
    "outputId": "be5d4fe8-619e-49ee-cc4f-6499cc3fd752"
   },
   "outputs": [],
   "source": [
    "# Distribution of Target variable (SalePrice)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.distplot(df_combined_train['SalePrice'],hist_kws={\"edgecolor\": (1,0,0,1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T05:49:00.301071Z",
     "iopub.status.busy": "2022-06-14T05:49:00.300560Z",
     "iopub.status.idle": "2022-06-14T05:49:00.309898Z",
     "shell.execute_reply": "2022-06-14T05:49:00.308602Z",
     "shell.execute_reply.started": "2022-06-14T05:49:00.301022Z"
    },
    "id": "ZGr_GyX1TVAF",
    "outputId": "8774430f-a7e2-4d58-b07e-cae649d04bad"
   },
   "outputs": [],
   "source": [
    "# Skew and kurtosis for SalePrice \n",
    "print(\"Skewness: %f\" % df_combined_train['SalePrice'].skew())\n",
    "print(\"Kurtosis: %f\" % df_combined_train['SalePrice'].kurt())\n",
    "\n",
    "# Hence, the skewness AND kurtosis have naturally declined by a bit on increasing the number of training instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T06:16:51.925495Z",
     "iopub.status.busy": "2022-06-14T06:16:51.925110Z",
     "iopub.status.idle": "2022-06-14T06:16:51.935551Z",
     "shell.execute_reply": "2022-06-14T06:16:51.934568Z",
     "shell.execute_reply.started": "2022-06-14T06:16:51.925464Z"
    },
    "id": "VkLsbXEbozb6"
   },
   "outputs": [],
   "source": [
    "# Applying log transformation to remove skewness and make target variable normally distributed(we apply natural log here)\n",
    "# Create a copy of training AND test datasets to apply transformations\n",
    "df_train_copy = df_combined_train.copy()\n",
    "y = df_combined_train['SalePrice']\n",
    "df_test_copy = df_test.copy()\n",
    "\n",
    "# Don't just copy the column names as the order is different \n",
    "# df_test_copy.columns = df_train_copy.columns\n",
    "\n",
    "df_train_copy['SalePrice'] = np.log1p(df_combined_train['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T09:45:45.267839Z",
     "iopub.status.busy": "2022-06-09T09:45:45.267487Z",
     "iopub.status.idle": "2022-06-09T09:45:45.278407Z",
     "shell.execute_reply": "2022-06-09T09:45:45.277157Z",
     "shell.execute_reply.started": "2022-06-09T09:45:45.267809Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reset the index for test data\n",
    "# df_test_copy.set_index('Id', inplace=True)\n",
    "# df_test_copy.index.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T05:49:14.225018Z",
     "iopub.status.busy": "2022-06-14T05:49:14.224204Z",
     "iopub.status.idle": "2022-06-14T05:49:14.494057Z",
     "shell.execute_reply": "2022-06-14T05:49:14.492663Z",
     "shell.execute_reply.started": "2022-06-14T05:49:14.224965Z"
    },
    "id": "8tZ7zwMhpe1x",
    "outputId": "3314ec16-73b5-4872-c989-3c923aca36a4"
   },
   "outputs": [],
   "source": [
    "# Distribution of Target variable (SalePrice) - again to see if the skewness has decreased \n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "# hist_kws parameter refers to edgecolour of bins in histogram\n",
    "sns.distplot(df_combined_train['SalePrice'], hist_kws={\"edgecolor\": (1,0,0,1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T05:49:18.134350Z",
     "iopub.status.busy": "2022-06-14T05:49:18.133889Z",
     "iopub.status.idle": "2022-06-14T05:49:18.143467Z",
     "shell.execute_reply": "2022-06-14T05:49:18.141823Z",
     "shell.execute_reply.started": "2022-06-14T05:49:18.134291Z"
    }
   },
   "outputs": [],
   "source": [
    "# Again calculate the skewness and kurtosis\n",
    "print(\"Skewness: \" + str(df_train_copy['SalePrice'].skew()))\n",
    "print(\"Kurtosis: \" + str(df_train_copy['SalePrice'].kurtosis()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***As we can observe, the skewness and kurtosis values after the requisite transformations have minimied and the distribution is almost normal***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T05:49:24.251665Z",
     "iopub.status.busy": "2022-06-14T05:49:24.251298Z",
     "iopub.status.idle": "2022-06-14T05:49:25.692958Z",
     "shell.execute_reply": "2022-06-14T05:49:25.692034Z",
     "shell.execute_reply.started": "2022-06-14T05:49:24.251632Z"
    },
    "id": "fO49NMBlpuql",
    "outputId": "a2ee32ed-6c36-47e3-bffe-2e3cec8897b4"
   },
   "outputs": [],
   "source": [
    "# Plotting the Pearsson's correlation heatmap between the numerical features\n",
    "# The code has been derived from Seaborn API docs.\n",
    "plt.figure(figsize=(30, 20))\n",
    "corr = df_numerical_train.corr()\n",
    "mask = np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "with sns.axes_style(\"white\"):\n",
    "    f, ax = plt.subplots(figsize=(20, 20))\n",
    "    ax = sns.heatmap(corr, mask=mask, vmax=.3, square=True, cmap='RdPu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBD6azpg3qMO"
   },
   "source": [
    "Hence we can see there is a high degree of multicollinearity in the dataset. We can either drop them by performing certain feature selection techniques or let some regularization ML techniques(such as Lasso and Ridge regression), do the needful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEo9g4RT5MOc"
   },
   "source": [
    "**We will perform some advanced Feature Engineering techniques such as(if required):**\n",
    "1. Outlier Detection and removal(for extremely skewed distributions of features)\n",
    "2. Missing Value Imputation\n",
    "3. Scaling and Normalization(if required)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T05:49:27.166448Z",
     "iopub.status.busy": "2022-06-14T05:49:27.166013Z",
     "iopub.status.idle": "2022-06-14T05:49:27.217890Z",
     "shell.execute_reply": "2022-06-14T05:49:27.216326Z",
     "shell.execute_reply.started": "2022-06-14T05:49:27.166413Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualization of categorical variables\n",
    "# cat_cols = [col for col in df_train_copy.columns if df_train_copy[col].dtype=='object']\n",
    "# df_cat_subset = df_train_copy[cat_cols]\n",
    "df_categorical_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T09:45:47.138543Z",
     "iopub.status.busy": "2022-06-09T09:45:47.138063Z",
     "iopub.status.idle": "2022-06-09T09:45:47.225827Z",
     "shell.execute_reply": "2022-06-09T09:45:47.224895Z",
     "shell.execute_reply.started": "2022-06-09T09:45:47.138472Z"
    }
   },
   "outputs": [],
   "source": [
    "# Counting VALUE COUNTS for categorical columns\n",
    "for col in df_categorical_train.columns:\n",
    "    print(str(col) + \"-\"*len(str(col)) + str(df_categorical_train[col].value_counts()))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T09:45:47.227868Z",
     "iopub.status.busy": "2022-06-09T09:45:47.227427Z",
     "iopub.status.idle": "2022-06-09T09:45:47.232491Z",
     "shell.execute_reply": "2022-06-09T09:45:47.231452Z",
     "shell.execute_reply.started": "2022-06-09T09:45:47.227826Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop 'Utilities' and 'Pave' columns as it has extremely low cardinality, and possibly, the model cannot learn much from it \n",
    "# df_train_copy.drop(['Street', 'Utilities'], axis=1, inplace=True)\n",
    "# We will decide upon the other features later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T09:45:47.234583Z",
     "iopub.status.busy": "2022-06-09T09:45:47.234207Z",
     "iopub.status.idle": "2022-06-09T09:45:47.247098Z",
     "shell.execute_reply": "2022-06-09T09:45:47.245868Z",
     "shell.execute_reply.started": "2022-06-09T09:45:47.23455Z"
    }
   },
   "outputs": [],
   "source": [
    "# Numerical columns \n",
    "# For the time being, as the percentage of missing values is extremely large for four features, namely ['PoolQc', 'MiscFeature', 'Alley' AND 'Fence'], we would drop them from training and test datasets\n",
    "# If appropriate information regarding these columns arrives, we would impute them accordingly \n",
    "# drop_features = ['PoolQC', 'MiscFeature', 'Alley', 'Fence']\n",
    "# df_train_copy.drop(drop_features, axis=1, inplace=True)\n",
    "# df_test_copy.drop(drop_features, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T06:17:04.276724Z",
     "iopub.status.busy": "2022-06-14T06:17:04.276065Z",
     "iopub.status.idle": "2022-06-14T06:17:04.285812Z",
     "shell.execute_reply": "2022-06-14T06:17:04.284694Z",
     "shell.execute_reply.started": "2022-06-14T06:17:04.276682Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's separate out the target variable \n",
    "df_train_copy = df_train_copy.drop(['SalePrice'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T09:45:47.263049Z",
     "iopub.status.busy": "2022-06-09T09:45:47.262752Z",
     "iopub.status.idle": "2022-06-09T09:45:47.274249Z",
     "shell.execute_reply": "2022-06-09T09:45:47.273081Z",
     "shell.execute_reply.started": "2022-06-09T09:45:47.26302Z"
    },
    "id": "peWa6Tou4Vrs",
    "outputId": "e5b93fd0-aa5a-4516-be29-3ef557735653"
   },
   "outputs": [],
   "source": [
    "# quantile75 = df_train_copy['GrLivArea'].quantile(0.75)\n",
    "# quantile25 = df_train_copy['GrLivArea'].quantile(0.25)\n",
    "# IQR = quantile75 - quantile25\n",
    "# IQR\n",
    "# print(IQR)\n",
    "# print()\n",
    "\n",
    "# fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "# plt.subplots_adjust(top=1.5, bottom=0.5, wspace=1)\n",
    "# fig.figsize=[50, 15]\n",
    "\n",
    "# Plot the BOXPLOT for this feature \n",
    "# df_train_copy['GrLivArea'].plot(kind='box', ax=ax1) \n",
    "\n",
    "\n",
    "# Plot the KDEplot for this feature\n",
    "# df_train_copy['GrLivArea'].plot(kind='kde', ax=ax2)\n",
    "# ax2.set_xlabel('GrLivArea')\n",
    "# print()\n",
    "\n",
    "# Print Skewness\n",
    "# print(df_train_copy['GrLivArea'].skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T09:45:47.277901Z",
     "iopub.status.busy": "2022-06-09T09:45:47.277479Z",
     "iopub.status.idle": "2022-06-09T09:45:47.286038Z",
     "shell.execute_reply": "2022-06-09T09:45:47.284913Z",
     "shell.execute_reply.started": "2022-06-09T09:45:47.277867Z"
    }
   },
   "outputs": [],
   "source": [
    "# Count the number of values in GrLivArea > 3000\n",
    "# print(df_train_copy[df_train_copy['GrLivArea']>3000].shape)\n",
    "\n",
    "# Since there are only 18 of them, let's cap them altogether with 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T09:45:47.287971Z",
     "iopub.status.busy": "2022-06-09T09:45:47.287668Z",
     "iopub.status.idle": "2022-06-09T09:45:47.29837Z",
     "shell.execute_reply": "2022-06-09T09:45:47.297323Z",
     "shell.execute_reply.started": "2022-06-09T09:45:47.287942Z"
    },
    "id": "3UZPpz4_4VgQ",
    "outputId": "80741ebf-998c-4357-fd0d-c385e52c4c5f"
   },
   "outputs": [],
   "source": [
    "# Here we would cap the outliers above , as they contribute to the data skewness\n",
    "# extreme_upper = df_train_copy['GrLivArea'].quantile(0.75) + 3 * IQR\n",
    "# df_train_copy['GrLivArea'] = np.where(df_train_copy['GrLivArea'] > extreme_upper, extreme_upper, df_train_copy['GrLivArea'])\n",
    "# df_train_copy['GrLivArea'] = np.where(df_train_copy['GrLivArea']>3000, 3000, df_train_copy['GrLivArea'])\n",
    "\n",
    "# Again plotting KDE plot \n",
    "# df_train_copy['GrLivArea'].plot(kind='kde')\n",
    "# print(df_train_copy['GrLivArea'].skew())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Hence we have created an approximate normal distribution***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T09:45:47.300232Z",
     "iopub.status.busy": "2022-06-09T09:45:47.299863Z",
     "iopub.status.idle": "2022-06-09T09:45:47.313332Z",
     "shell.execute_reply": "2022-06-09T09:45:47.312267Z",
     "shell.execute_reply.started": "2022-06-09T09:45:47.30019Z"
    },
    "id": "f45-rzNdWmO9"
   },
   "outputs": [],
   "source": [
    "# Let's replace the outliers in upper part of feature set with 75th quantile values\n",
    "# df_train_copy.loc[df_train_copy['GrLivArea']>=4500, 'GrLivArea'] = quantile25\n",
    "# Or you can also choose to drop these outlier values from dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T09:45:47.314893Z",
     "iopub.status.busy": "2022-06-09T09:45:47.3146Z",
     "iopub.status.idle": "2022-06-09T09:45:47.323824Z",
     "shell.execute_reply": "2022-06-09T09:45:47.322997Z",
     "shell.execute_reply.started": "2022-06-09T09:45:47.314864Z"
    },
    "id": "NjEBuakmXfLt",
    "outputId": "38cc80a5-a6d2-4ff6-eccd-f090b0e98f10"
   },
   "outputs": [],
   "source": [
    "# df_train_copy['GrLivArea'].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T09:45:47.325579Z",
     "iopub.status.busy": "2022-06-09T09:45:47.325008Z",
     "iopub.status.idle": "2022-06-09T09:45:47.33588Z",
     "shell.execute_reply": "2022-06-09T09:45:47.334608Z",
     "shell.execute_reply.started": "2022-06-09T09:45:47.325547Z"
    },
    "id": "l1iAaxEVYD9P"
   },
   "outputs": [],
   "source": [
    "# df_train_copy['age']=df_train_copy['YrSold']-df_train_copy['YearBuilt']\n",
    "\n",
    "# See why its been done like this\n",
    "# Some of the non-numeric predictors are stored as numbers; convert them into strings will convert those columns into dummy variables later.\n",
    "# df_train_copy['MSSubClass'] = df_train_copy['MSSubClass'].astype(str) \n",
    "# df_train_copy['YrSold'] = df_train_copy['YrSold'].astype(str) #year\n",
    "# df_train_copy['MoSold'] = df_train_copy['MoSold'].astype(str) #month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T06:17:14.386117Z",
     "iopub.status.busy": "2022-06-14T06:17:14.385365Z",
     "iopub.status.idle": "2022-06-14T06:17:14.394174Z",
     "shell.execute_reply": "2022-06-14T06:17:14.392756Z",
     "shell.execute_reply.started": "2022-06-14T06:17:14.386073Z"
    }
   },
   "outputs": [],
   "source": [
    "# According to the description, this column refers to the Linear Street of feet. We can't impute using 0. Let's try with 'mean' or 'median' first \n",
    "df_train_copy['LotFrontage_median'] = df_train_copy['LotFrontage'].fillna(df_train_copy['LotFrontage'].dropna().median())\n",
    "\n",
    "\n",
    "# Since we have access to test data, we would impute with values from test data. Else impute with values from training data\n",
    "df_test_copy['LotFrontage_median'] = df_test_copy['LotFrontage'].fillna(df_test_copy['LotFrontage'].dropna().median())\n",
    "\n",
    "# Let's study the KDE distribution of this feature \n",
    "# df_train_copy['LotFrontage_median'].plot(kind='kde')\n",
    "\n",
    "# Printing the skewness and krutosis of this distribution \n",
    "# print(df_train_copy['LotFrontage_median'].skew())\n",
    "# print(df_train_copy['LotFrontage_median'].kurtosis())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T06:17:15.986660Z",
     "iopub.status.busy": "2022-06-14T06:17:15.985677Z",
     "iopub.status.idle": "2022-06-14T06:17:16.241389Z",
     "shell.execute_reply": "2022-06-14T06:17:16.240425Z",
     "shell.execute_reply.started": "2022-06-14T06:17:15.986602Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's fill it using random imputation function\n",
    "df_train_copy['LotFrontage_fill'] = df_train_copy['LotFrontage']\n",
    "random_sample = df_train_copy['LotFrontage'].dropna().sample(df_train_copy['LotFrontage'].isnull().sum(), random_state=seed_value)\n",
    "random_sample.index = df_train_copy[df_train_copy['LotFrontage'].isnull()].index\n",
    "df_train_copy.loc[df_train_copy['LotFrontage_fill'].isnull(), 'LotFrontage_fill'] = random_sample\n",
    "\n",
    "# Perform the same on test data\n",
    "df_test_copy['LotFrontage_fill'] = df_test_copy['LotFrontage']\n",
    "random_sample = df_test_copy['LotFrontage'].dropna().sample(df_test_copy['LotFrontage'].isnull().sum(), random_state=seed_value)\n",
    "random_sample.index = df_test_copy[df_test_copy['LotFrontage'].isnull()].index\n",
    "df_test_copy.loc[df_test_copy['LotFrontage_fill'].isnull(), 'LotFrontage_fill'] = random_sample\n",
    "\n",
    "# Let's study the KDE plot for this variable \n",
    "df_train_copy['LotFrontage_fill'].round(0).plot(kind='kde')\n",
    "\n",
    "# Again study kurtosis\n",
    "print(df_train_copy['LotFrontage_fill'].round(0).kurtosis())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T05:50:26.410384Z",
     "iopub.status.busy": "2022-06-14T05:50:26.409989Z",
     "iopub.status.idle": "2022-06-14T05:50:26.641801Z",
     "shell.execute_reply": "2022-06-14T05:50:26.640275Z",
     "shell.execute_reply.started": "2022-06-14T05:50:26.410345Z"
    }
   },
   "outputs": [],
   "source": [
    "# Studying GarageYr. Belt\n",
    "df_train_copy['GarageYrBlt'].plot(kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Value Imputation + Encoding Categorical Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T06:17:20.301128Z",
     "iopub.status.busy": "2022-06-14T06:17:20.300738Z",
     "iopub.status.idle": "2022-06-14T06:17:20.310376Z",
     "shell.execute_reply": "2022-06-14T06:17:20.309232Z",
     "shell.execute_reply.started": "2022-06-14T06:17:20.301095Z"
    }
   },
   "outputs": [],
   "source": [
    "# Again define numerical and categorical columns as per the new dataset \n",
    "num_cols = [col for col in df_train_copy.columns if df_train_copy[col].dtype != 'object']\n",
    "cat_cols = [col for col in df_train_copy.columns if col not in num_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T06:17:23.455871Z",
     "iopub.status.busy": "2022-06-14T06:17:23.455477Z",
     "iopub.status.idle": "2022-06-14T06:17:23.549635Z",
     "shell.execute_reply": "2022-06-14T06:17:23.548713Z",
     "shell.execute_reply.started": "2022-06-14T06:17:23.455803Z"
    },
    "id": "Hy34II3E0at3"
   },
   "outputs": [],
   "source": [
    "# Functional: Home functionality (Assume typical unless deductions are warranted)\n",
    "df_train_copy['Functional'] = df_train_copy['Functional'].fillna('Typ')\n",
    "df_test_copy['Functional'] = df_test_copy['Functional'].fillna('Typ')\n",
    "\n",
    "df_train_copy['Electrical'] = df_train_copy['Electrical'].fillna('SBrkr') #Filling with modef\n",
    "df_test_copy['Electrical'] = df_test_copy['Electrical'].fillna('SBrkr')\n",
    "\n",
    "# data description states that NA refers to \"No Pool\"\n",
    "df_train_copy['PoolQC'] = df_train_copy['PoolQC'].fillna('Missing')\n",
    "df_test_copy['PoolQC'] = df_test_copy['PoolQC'].fillna('Missing')\n",
    "\n",
    "# Replacing the missing values with 0, since no garage = no cars in garage inferred from data dictionary\n",
    "df_train_copy['GarageYrBlt'] = df_train_copy['GarageYrBlt'].fillna(0)\n",
    "df_test_copy['GarageYrBlt'] = df_test_copy['GarageYrBlt'].fillna(0)\n",
    "\n",
    "# Filling missing values in KitchenQuality with 'TA', meaning average  \n",
    "df_train_copy['KitchenQual'] = df_train_copy['KitchenQual'].fillna('TA')\n",
    "df_test_copy['KitchenQual'] = df_test_copy['KitchenQual'].fillna('TA')\n",
    "\n",
    "# Filling missing values with mode\n",
    "df_train_copy['Exterior1st'] = df_train_copy['Exterior1st'].fillna(df_train_copy['Exterior1st'].mode()[0])\n",
    "df_test_copy['Exterior1st'] = df_test_copy['Exterior1st'].fillna(df_test_copy['Exterior1st'].mode()[0])\n",
    "\n",
    "df_train_copy['Exterior2nd'] = df_train_copy['Exterior2nd'].fillna(df_train_copy['Exterior2nd'].mode()[0])\n",
    "df_test_copy['Exterior2nd'] = df_test_copy['Exterior2nd'].fillna(df_test_copy['Exterior2nd'].mode()[0])\n",
    "\n",
    "df_train_copy['SaleType'] = df_train_copy['SaleType'].fillna(df_train_copy['SaleType'].mode()[0])\n",
    "df_test_copy['SaleType'] = df_test_copy['SaleType'].fillna(df_test_copy['SaleType'].mode()[0])\n",
    "\n",
    "# None means no Garage\n",
    "for col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n",
    "     df_train_copy[col] = df_train_copy[col].fillna('Missing')\n",
    "     df_test_copy[col] = df_test_copy[col].fillna('Missing')\n",
    "\n",
    "# None means no Basement        \n",
    "for col in ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']:\n",
    "    df_train_copy[col] = df_train_copy[col].fillna('Missing')\n",
    "    df_test_copy[col] = df_test_copy[col].fillna('Missing')\n",
    "\n",
    "df_train_copy['MSZoning'] = df_train_copy.groupby('MSSubClass')['MSZoning'].transform(lambda x : x.fillna(x.mode()[0]))\n",
    "df_test_copy['MSZoning'] = df_test_copy.groupby('MSSubClass')['MSZoning'].transform(lambda x : x.fillna(x.mode()[0]))\n",
    "# The pandas update function is used to 'Modify in place using non-NA values from another DataFrame. Aligns on indices. There is no return value.'\n",
    "# All of the above is stated according to documentation.\n",
    "\n",
    "# df_train_copy[cat_cols].update(df_train_copy[cat_cols].fillna(\"Missing\"))\n",
    "# df_train_copy[num_cols].update(df_train_copy[num_cols].fillna(0))\n",
    "# df_test_copy[num_cols].fillna(\"Missing\", inplace=True)\n",
    "\n",
    "# Update Missing LotFrontage values with 0\n",
    "df_train_copy['LotFrontage'] = df_train_copy['LotFrontage'].fillna(0)\n",
    "df_test_copy['LotFrontage'] = df_test_copy['LotFrontage'].fillna(0)\n",
    "\n",
    "\n",
    "# Update Missing 'Fireplace QA' wirh 'NA' feature\n",
    "df_train_copy['FireplaceQu'] = df_train_copy['FireplaceQu'].fillna('NA')\n",
    "df_test_copy['FireplaceQu'] = df_test_copy['FireplaceQu'].fillna('NA')\n",
    "\n",
    "\n",
    "# Updating 'MasVnrType'(None) and 'MasVnrArea'(0)\n",
    "df_train_copy['MasVnrType'] = df_train_copy['MasVnrType'].fillna('None')\n",
    "df_test_copy['MasVnrType'] = df_test_copy['MasVnrType'].fillna('None')\n",
    "\n",
    "df_train_copy['MasVnrArea'] = df_train_copy['MasVnrArea'].fillna(0)\n",
    "df_test_copy['MasVnrArea'] = df_test_copy['MasVnrArea'].fillna(0)\n",
    "\n",
    "\n",
    "# Update 'Utilities' column\n",
    "for col in ['Utilities', 'Exterior2nd']:\n",
    "    df_train_copy[col] = df_train_copy[col].fillna('Missing')\n",
    "    df_test_copy[col] = df_test_copy[col].fillna('Missing')\n",
    "    \n",
    "# Fill in missing values \n",
    "for col in ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath']:\n",
    "    df_train_copy[col] = df_train_copy[col].fillna(0)\n",
    "    df_test_copy[col] = df_test_copy[col].fillna(0)\n",
    "\n",
    "# Drop ['Alley', 'Fence' and 'MiscFeature']\n",
    "df_train_copy.drop(['Alley', 'Fence', 'MiscFeature'], axis=1, inplace=True)\n",
    "df_test_copy.drop(['Alley', 'Fence', 'MiscFeature'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T06:17:28.431380Z",
     "iopub.status.busy": "2022-06-14T06:17:28.431013Z",
     "iopub.status.idle": "2022-06-14T06:17:28.447486Z",
     "shell.execute_reply": "2022-06-14T06:17:28.445956Z",
     "shell.execute_reply.started": "2022-06-14T06:17:28.431347Z"
    }
   },
   "outputs": [],
   "source": [
    "# Finally IMPUTE some more randomly missing values with 0\n",
    "df_train_copy['BsmtFinSF1'] = df_train_copy['BsmtFinSF1'].fillna(0)\n",
    "df_test_copy['BsmtFinSF1'] = df_test_copy['BsmtFinSF1'].fillna(0)\n",
    "\n",
    "df_train_copy['BsmtFinSF2'] = df_train_copy['BsmtFinSF2'].fillna(0)\n",
    "df_test_copy['BsmtFinSF2'] = df_test_copy['BsmtFinSF2'].fillna(0)\n",
    "\n",
    "df_train_copy['BsmtUnfSF'] = df_train_copy['BsmtUnfSF'].fillna(0)\n",
    "df_test_copy['BsmtUnfSF'] = df_test_copy['BsmtUnfSF'].fillna(0)\n",
    "\n",
    "df_train_copy['TotalBsmtSF'] = df_train_copy['TotalBsmtSF'].fillna(0)\n",
    "df_train_copy['TotalBsmtSF'] = df_train_copy['TotalBsmtSF'].fillna(0)\n",
    "\n",
    "df_train_copy['GarageCars'] = df_train_copy['GarageCars'].fillna(0)\n",
    "df_test_copy['GarageCars'] = df_test_copy['GarageCars'].fillna(0)\n",
    "\n",
    "df_train_copy['GarageArea'] = df_train_copy['GarageCars'].fillna(0)\n",
    "df_test_copy['GarageArea'] = df_test_copy['GarageArea'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Imputing Categorical Features***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T06:17:34.073661Z",
     "iopub.status.busy": "2022-06-14T06:17:34.073295Z",
     "iopub.status.idle": "2022-06-14T06:17:34.156942Z",
     "shell.execute_reply": "2022-06-14T06:17:34.155914Z",
     "shell.execute_reply.started": "2022-06-14T06:17:34.073631Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use these imputations later on if required. Use these features, add them to the model else use them in conjunction for feature aggregation\n",
    "df_train_copy['PoolArea'+'_impute'] = df_train_copy['PoolArea'].apply(lambda x : 1 if x==0 else 0)\n",
    "df_train_copy['MiscVal'+'_impute'] = df_train_copy['MiscVal'].apply(lambda x : 1 if x==0 else 0)\n",
    "df_train_copy['ScreenPorch'+'_impute'] = df_train_copy['ScreenPorch'].apply(lambda x : 1 if x==0 else 0)\n",
    "df_train_copy['3SsnPorch'+'_impute'] = df_train_copy['3SsnPorch'].apply(lambda x : 1 if x==0 else 0)\n",
    "df_train_copy['EnclosedPorch'+'_impute'] = df_train_copy['EnclosedPorch'].apply(lambda x : 1 if x==0 else 0)\n",
    "df_train_copy['WoodDeckSF'+'_impute'] = df_train_copy['WoodDeckSF'].apply(lambda x : 1 if x>0 else 0)\n",
    "df_train_copy['OpenPorchSF'+'_impute'] = df_train_copy['OpenPorchSF'].apply(lambda x : 1 if x>0 else 0)\n",
    "df_train_copy['HalfBath'+'_impute'] = df_train_copy['HalfBath'].apply(lambda x : 1 if x==0 else 0)\n",
    "df_train_copy['Fireplaces'+'_impute'] = df_train_copy['Fireplaces'].apply(lambda x : 1 if x==0 else 0)\n",
    "df_train_copy['Fireplaces'+'_impute'] = df_train_copy['Fireplaces'].apply(lambda x : 1 if x>0 else 0)\n",
    "df_train_copy['2ndFlrSF'+'_impute'] = df_train_copy['2ndFlrSF'].apply(lambda x : 1 if x==0 else 0)\n",
    "df_train_copy['LowQualFinSF'+'_impute'] = df_train_copy['LowQualFinSF'].apply(lambda x : 1 if x==0 else 0)\n",
    "\n",
    "# Follow the same procedure for test columns \n",
    "df_test_copy['PoolArea'+'_impute'] = df_test_copy['PoolArea'].apply(lambda x : 1 if x==0 else 0)\n",
    "df_test_copy['MiscVal'+'_impute'] = df_test_copy['MiscVal'].apply(lambda x : 1 if x==0 else 0)\n",
    "df_test_copy['ScreenPorch'+'_impute'] = df_test_copy['ScreenPorch'].apply(lambda x : 1 if x==0 else 0)\n",
    "df_test_copy['3SsnPorch'+'_impute'] = df_test_copy['3SsnPorch'].apply(lambda x : 1 if x==0 else 0)\n",
    "df_test_copy['EnclosedPorch'+'_impute'] = df_test_copy['EnclosedPorch'].apply(lambda x : 1 if x==0 else 0)\n",
    "df_test_copy['WoodDeckSF'+'_impute'] = df_test_copy['WoodDeckSF'].apply(lambda x : 1 if x>0 else 0)\n",
    "df_test_copy['OpenPorchSF'+'_impute'] = df_test_copy['OpenPorchSF'].apply(lambda x : 1 if x>0 else 0)\n",
    "df_test_copy['HalfBath'+'_impute'] = df_test_copy['HalfBath'].apply(lambda x : 1 if x==0 else 0)\n",
    "df_test_copy['Fireplaces'+'_impute'] = df_test_copy['Fireplaces'].apply(lambda x : 1 if x==0 else 0)\n",
    "df_test_copy['Fireplaces'+'_impute'] = df_test_copy['Fireplaces'].apply(lambda x : 1 if x>0 else 0)\n",
    "df_test_copy['2ndFlrSF'+'_impute'] = df_test_copy['2ndFlrSF'].apply(lambda x : 1 if x==0 else 0)\n",
    "df_test_copy['LowQualFinSF'+'_impute'] = df_test_copy['LowQualFinSF'].apply(lambda x : 1 if x==0 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***AWARE***\n",
    "1. The columns have been randomly imputed based on the count of specific features in the dataset. This method is a very simple version to compute missing values which gives importance to 'Missing' values if there is a significant count of this variable, else rest of the features are given equal weighatge and encoded as '1'\n",
    "2. Here the missing values are not at random(MNAR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T06:17:38.350871Z",
     "iopub.status.busy": "2022-06-14T06:17:38.350168Z",
     "iopub.status.idle": "2022-06-14T06:17:38.361232Z",
     "shell.execute_reply": "2022-06-14T06:17:38.359707Z",
     "shell.execute_reply.started": "2022-06-14T06:17:38.350830Z"
    }
   },
   "outputs": [],
   "source": [
    "num_cols_1 = [col for col in df_train_copy.columns if df_train_copy[col].dtype!='object' and col!='SalePrice']\n",
    "cat_cols_1 = [col for col in df_train_copy.columns if col not in num_cols_1 and col!='SalePrice']\n",
    "num_cols_1.remove('YearRemod/Add')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T09:45:47.966721Z",
     "iopub.status.busy": "2022-06-09T09:45:47.966388Z",
     "iopub.status.idle": "2022-06-09T09:45:47.981099Z",
     "shell.execute_reply": "2022-06-09T09:45:47.979833Z",
     "shell.execute_reply.started": "2022-06-09T09:45:47.966691Z"
    },
    "id": "7Wb8-siNreSD",
    "outputId": "c2693694-acb0-46e2-d080-dbe9ff0e9371"
   },
   "outputs": [],
   "source": [
    "# We have already removed skewness from target variable, we need to check out the skewness among various features too\n",
    "# df_train_copy_num = df_train_copy.select_dtypes(['int64', 'float64'])\n",
    "# skew_features = df_train_copy_num.apply(lambda x : x.skew()).sort_values(ascending=False)\n",
    "\n",
    "# skew_high = skew_features[skew_features > 0.6] \n",
    "# This command returns a series\n",
    "\n",
    "# high_indices = skew_high.index\n",
    "# This returns a list of indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T09:45:47.983345Z",
     "iopub.status.busy": "2022-06-09T09:45:47.982879Z",
     "iopub.status.idle": "2022-06-09T09:45:47.993346Z",
     "shell.execute_reply": "2022-06-09T09:45:47.991905Z",
     "shell.execute_reply.started": "2022-06-09T09:45:47.983299Z"
    },
    "id": "tQvUrIXg1cwf",
    "outputId": "8f87085d-3e66-4d7e-eeef-771013cbf1fc"
   },
   "outputs": [],
   "source": [
    "# Refer this for boxplot(numerical value distribution)\n",
    "# fig, ax = plt.subplots(figsize=(8, 7))\n",
    "# ax.set_xscale(\"log\")\n",
    "# ax = sns.boxplot(data=df_train_copy_num , orient=\"h\", palette=\"Set1\")\n",
    "# ax.xaxis.grid(False)\n",
    "# ax.set(ylabel=\"FeatureNames\")\n",
    "# ax.set(xlabel=\"Numeric values\")\n",
    "# ax.set(title=\"Numeric Distribution of Features\")\n",
    "# sns.despine(trim=True, left=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T09:45:47.995315Z",
     "iopub.status.busy": "2022-06-09T09:45:47.994795Z",
     "iopub.status.idle": "2022-06-09T09:45:48.00736Z",
     "shell.execute_reply": "2022-06-09T09:45:48.006044Z",
     "shell.execute_reply.started": "2022-06-09T09:45:47.995271Z"
    },
    "id": "0Ye2DLM94Bqk"
   },
   "outputs": [],
   "source": [
    "# Normalize skewed features using a box-cox normal distribution, we can surely use other techniques but it works very well on this dataset\n",
    "# Check out for other techniques used to normalize skewed features(sum of them being)\n",
    "# People usually use box-cox and StandardScaler for removing skewed data\n",
    "# for i in high_indices:\n",
    "      # What's this 1.002 used for?\n",
    "#     df_train_copy[i] = boxcox1p(df_train_copy[i], boxcox_normmax(df_train_copy[i] + 1.002))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T09:45:48.010142Z",
     "iopub.status.busy": "2022-06-09T09:45:48.009688Z",
     "iopub.status.idle": "2022-06-09T09:45:48.018883Z",
     "shell.execute_reply": "2022-06-09T09:45:48.017944Z",
     "shell.execute_reply.started": "2022-06-09T09:45:48.01011Z"
    },
    "id": "WnWj_HRBmtta",
    "outputId": "955a655d-b88f-4807-8317-05714b2be472"
   },
   "outputs": [],
   "source": [
    "# Creating more features by log transformation\n",
    "# Refer this for boxplot(numerical value distribution)\n",
    "# fig, ax = plt.subplots(figsize=(8, 7))\n",
    "# ax.set_xscale(\"log\")\n",
    "# ax = sns.boxplot(data=df_train_copy_num[high_indices] , orient=\"h\", palette=\"Set1\")\n",
    "# ax.xaxis.grid(False)\n",
    "# ax.set(ylabel=\"Feature Names\")\n",
    "# ax.set(xlabel=\"Numeric values\")\n",
    "# ax.set(title=\"Numeric Distribution of Features\")\n",
    "# sns.despine(trim=True, left=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T09:45:48.020174Z",
     "iopub.status.busy": "2022-06-09T09:45:48.0199Z",
     "iopub.status.idle": "2022-06-09T09:45:48.032813Z",
     "shell.execute_reply": "2022-06-09T09:45:48.03174Z",
     "shell.execute_reply.started": "2022-06-09T09:45:48.020148Z"
    },
    "id": "BxhR8QMpo5qX"
   },
   "outputs": [],
   "source": [
    "# from scipy import stats\n",
    "# high_indices_list = list(high_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T09:45:48.034873Z",
     "iopub.status.busy": "2022-06-09T09:45:48.034557Z",
     "iopub.status.idle": "2022-06-09T09:45:48.044412Z",
     "shell.execute_reply": "2022-06-09T09:45:48.04354Z",
     "shell.execute_reply.started": "2022-06-09T09:45:48.034841Z"
    },
    "id": "YWLsiKfGonxZ"
   },
   "outputs": [],
   "source": [
    "# nrows = 11\n",
    "# ncols = 2\n",
    "\n",
    "# fig, axes = plt.subplots(nrows, ncols, figsize=(20, 15))\n",
    "\n",
    "# #Initializing lazy counter\n",
    "# count = 0\n",
    "\n",
    "# for i in range(nrows):\n",
    "#   for j in range(ncols):\n",
    "#       ax = axes[i, j]\n",
    "\n",
    "#       if count < len(high_indices.tolist()):\n",
    "#         ax.plot(df_train_copy[high_indices_list[count]])\n",
    "#         ax.set(xlabel=\"Feature Names\")\n",
    "#         ax.title(\"Skewness distribution of a feature variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T09:45:48.0461Z",
     "iopub.status.busy": "2022-06-09T09:45:48.04567Z",
     "iopub.status.idle": "2022-06-09T09:45:48.061812Z",
     "shell.execute_reply": "2022-06-09T09:45:48.060688Z",
     "shell.execute_reply.started": "2022-06-09T09:45:48.046056Z"
    },
    "id": "A8NAHxs4tuPP",
    "outputId": "08b6f89d-978c-4a1b-e40d-1a1ea5412708"
   },
   "outputs": [],
   "source": [
    "# NOt useful columns in our predictions, more than 99% rows have same value.\n",
    "# print(df_train_copy['Utilities'].value_counts())\n",
    "# NOt useful columns in our predictions, more than 99% rows have same value.\n",
    "# print(df_train_copy['Street'].value_counts())\n",
    "# NOt useful columns in our predictions, more than 99% rows have same value.\n",
    "# print(df_train_copy['PoolQC'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T09:45:48.063887Z",
     "iopub.status.busy": "2022-06-09T09:45:48.063387Z",
     "iopub.status.idle": "2022-06-09T09:45:48.074921Z",
     "shell.execute_reply": "2022-06-09T09:45:48.073987Z",
     "shell.execute_reply.started": "2022-06-09T09:45:48.063839Z"
    },
    "id": "0NPeZgzcFnOK"
   },
   "outputs": [],
   "source": [
    "# As we can see above, those columns have very little other useful data as their values and are primarily composed of a single feature. It would be better \n",
    "# if we drop them as they are adding up as a redundant feature without giving much insights about the data.\n",
    "# df_train_copy = df_train_copy.drop(['Utilities', 'Street', 'PoolQC'], axis=1)\n",
    "# df_test_copy = df_test_copy.drop(['Utilities', 'Street', 'PoolQC'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T09:45:48.07652Z",
     "iopub.status.busy": "2022-06-09T09:45:48.076067Z",
     "iopub.status.idle": "2022-06-09T09:45:48.087246Z",
     "shell.execute_reply": "2022-06-09T09:45:48.086402Z",
     "shell.execute_reply.started": "2022-06-09T09:45:48.07646Z"
    },
    "id": "OC7ndFgZPxCM",
    "outputId": "dce8969a-0230-449e-ca41-87844d754c73"
   },
   "outputs": [],
   "source": [
    "#The main difference between apply and transform functions is that while apply passes the dataframe in the form of columns to the custom function, \n",
    "#whereas the transform method passes the dataframe as a series to the custom function.\n",
    "#Let's check out the number of 0's in dataset\n",
    "# for col in numerical_cols:\n",
    "#   print(col, \"\\t\", len(list(df_train_copy.loc[df_train_copy[col] == 0, col].index)))\n",
    "#Hence we can observe there are many columns containing 0 as a value\n",
    "#We need some way to remove their unusefulness as they may be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T09:45:48.103457Z",
     "iopub.status.busy": "2022-06-09T09:45:48.10299Z",
     "iopub.status.idle": "2022-06-09T09:45:48.117571Z",
     "shell.execute_reply": "2022-06-09T09:45:48.116791Z",
     "shell.execute_reply.started": "2022-06-09T09:45:48.103426Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_train_copy['TotalBsmtSF'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T09:45:48.119088Z",
     "iopub.status.busy": "2022-06-09T09:45:48.11862Z",
     "iopub.status.idle": "2022-06-09T09:45:48.129197Z",
     "shell.execute_reply": "2022-06-09T09:45:48.12793Z",
     "shell.execute_reply.started": "2022-06-09T09:45:48.119053Z"
    },
    "id": "C3a9mYOI-cDt"
   },
   "outputs": [],
   "source": [
    "# Understand this feature descriptor later on. Right now focus on features that are in use \n",
    "# df_train_copy['TotalBsmtSF'] = df_train_copy['TotalBsmtSF'].apply(lambda x: np.exp(6) if x <= 0.0 else x)\n",
    "# df_train_copy['2ndFlrSF'] = df_train_copy['2ndFlrSF'].apply(lambda x: np.exp(6.5) if x <= 0.0 else x)\n",
    "# df_train_copy['LotFrontage'] = df_train_copy['LotFrontage'].apply(lambda x: np.exp(4.2) if x <= 0.0 else x)\n",
    "# df_train_copy['MasVnrArea'] = df_train_copy['MasVnrArea'].apply(lambda x: np.exp(4) if x <= 0.0 else x)\n",
    "# df_train_copy['BsmtFinSF1'] = df_train_copy['BsmtFinSF1'].apply(lambda x: np.exp(6.5) if x <= 0.0 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T09:45:48.130686Z",
     "iopub.status.busy": "2022-06-09T09:45:48.130346Z",
     "iopub.status.idle": "2022-06-09T09:45:48.140471Z",
     "shell.execute_reply": "2022-06-09T09:45:48.139708Z",
     "shell.execute_reply.started": "2022-06-09T09:45:48.130657Z"
    },
    "id": "ldub526cAL5r"
   },
   "outputs": [],
   "source": [
    "# Creating more features by log transformation\n",
    "# def log_transform(result, features):\n",
    "#   m = result.shape[1]\n",
    "\n",
    "#   for feature in features:\n",
    "    \n",
    "#     # The Pandas assign function assigns a new column to the dataframe with a modified feature. Look up the docs for further information.\n",
    "#     result = result.assign(newcol = pd.Series(np.log(1.01+result[feature])))\n",
    "#     # columns.values returns a numpy array\n",
    "#     result.columns.values[m] = feature + '_log' \n",
    "#     m += 1\n",
    "\n",
    "#   return result\n",
    "\n",
    "# log_features = ['LotFrontage','LotArea','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF',\n",
    "#                  'TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea',\n",
    "#                  'BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr',\n",
    "#                  'TotRmsAbvGrd','Fireplaces','GarageCars','GarageArea','WoodDeckSF','OpenPorchSF',\n",
    "#                  'EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal','YearRemodAdd']\n",
    "\n",
    "# df_train_copy = log_transform(df_train_copy, log_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJNR1klOK-ee"
   },
   "source": [
    "Now that every possible transformation of data has been taken care of, let's one-hot encode our categorical variables. That's as easy as it sounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T10:45:00.897296Z",
     "iopub.status.busy": "2022-06-09T10:45:00.896682Z",
     "iopub.status.idle": "2022-06-09T10:45:00.907132Z",
     "shell.execute_reply": "2022-06-09T10:45:00.905566Z",
     "shell.execute_reply.started": "2022-06-09T10:45:00.89726Z"
    },
    "id": "Wa_WPy9_LVyi"
   },
   "outputs": [],
   "source": [
    "# df_train_copy_num = df_train_copy.select_dtypes(include=['int64', 'float64'])\n",
    "# df_train_copy_cat = df_train_copy.select_dtypes(exclude=['int64', 'float64'])\n",
    "\n",
    "# df_train_copy_cat = pd.get_dummies(df_train_copy_cat, drop_first=True)\n",
    "\n",
    "# df_train_copy_final = pd.concat([df_train_copy_num, df_train_copy_cat], axis=1)\n",
    "# remove_cols = ['LotFrontage_median', 'PoolArea_impute', 'MiscVal_impute', 'ScreenPorch_impute', '3SsnPorch_impute', 'EnclosedPorch_impute', 'WoodDeckSF_impute', 'OpenPorchSF_impute', 'OpenPorchSF_impute', \n",
    "#            'HalfBath_impute', 'Fireplaces_impute', 'Fireplaces_impute', '2ndFlrSF_impute', 'LowQualFinSF_impute']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T06:18:17.866833Z",
     "iopub.status.busy": "2022-06-14T06:18:17.866174Z",
     "iopub.status.idle": "2022-06-14T06:18:17.877195Z",
     "shell.execute_reply": "2022-06-14T06:18:17.876100Z",
     "shell.execute_reply.started": "2022-06-14T06:18:17.866775Z"
    },
    "id": "Wa_WPy9_LVyi"
   },
   "outputs": [],
   "source": [
    "# Remove 'SalePrice' from train set features\n",
    "# y = df_origin['SalePrice']\n",
    "# num_cols_1_train.remove('SalePrice')\n",
    "train_data = df_train_copy[num_cols_1]\n",
    "test_data = df_test_copy[num_cols_1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "en4_k-rmlvyy"
   },
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T06:18:19.757026Z",
     "iopub.status.busy": "2022-06-14T06:18:19.756219Z",
     "iopub.status.idle": "2022-06-14T06:18:19.778801Z",
     "shell.execute_reply": "2022-06-14T06:18:19.776899Z",
     "shell.execute_reply.started": "2022-06-14T06:18:19.756908Z"
    },
    "id": "Ry3syNupjVi_",
    "outputId": "9f0a6c36-682c-40c9-9935-071e33cb7657"
   },
   "outputs": [],
   "source": [
    "# Remove FEATURES using Variance Threshold. Drop colums whose variability in values is less, that is, one feature dominates more than others. \n",
    "# This can lead to overfitting and hence, these columns must be dropped \n",
    "var_threshold = VarianceThreshold(threshold=0.2)\n",
    "var_threshold.fit_transform(train_data)\n",
    "\n",
    "var_threshold.get_support()\n",
    "# len(train_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T06:18:22.353166Z",
     "iopub.status.busy": "2022-06-14T06:18:22.352753Z",
     "iopub.status.idle": "2022-06-14T06:18:22.363747Z",
     "shell.execute_reply": "2022-06-14T06:18:22.362814Z",
     "shell.execute_reply.started": "2022-06-14T06:18:22.353133Z"
    },
    "id": "W7LMzhbIqTUq",
    "outputId": "e379d20f-c202-4448-83f9-c8b49ed3f62d"
   },
   "outputs": [],
   "source": [
    "less_variance_columns = [col for col in train_data.columns if col not in train_data.columns[var_threshold.get_support()]]\n",
    "train_data.drop(less_variance_columns, axis=1, inplace=True)\n",
    "test_data.drop(less_variance_columns, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T06:18:25.654097Z",
     "iopub.status.busy": "2022-06-14T06:18:25.653338Z",
     "iopub.status.idle": "2022-06-14T06:18:25.851840Z",
     "shell.execute_reply": "2022-06-14T06:18:25.850184Z",
     "shell.execute_reply.started": "2022-06-14T06:18:25.654054Z"
    },
    "id": "bGxcdWC0rQnQ"
   },
   "outputs": [],
   "source": [
    "# Plot the correlation matrix\n",
    "# Remove 'Id' column from numerical columns \n",
    "# numerical_cols.remove('Order')\n",
    "corr_matrix = train_data.corr()\n",
    "\n",
    "\n",
    "def highlight_cells(val):\n",
    "    color = 'yellow' if val >= 0.8 and val != 1 else ' '\n",
    "    return 'background-color: {}'.format(color)\n",
    "\n",
    "corr_matrix.style.applymap(highlight_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T05:51:35.175127Z",
     "iopub.status.busy": "2022-06-14T05:51:35.174449Z",
     "iopub.status.idle": "2022-06-14T05:51:35.234906Z",
     "shell.execute_reply": "2022-06-14T05:51:35.234050Z",
     "shell.execute_reply.started": "2022-06-14T05:51:35.175083Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T06:18:28.550635Z",
     "iopub.status.busy": "2022-06-14T06:18:28.549799Z",
     "iopub.status.idle": "2022-06-14T06:18:39.867216Z",
     "shell.execute_reply": "2022-06-14T06:18:39.865121Z",
     "shell.execute_reply.started": "2022-06-14T06:18:28.550595Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's plot a pairgrid of a subset of numerical features\n",
    "sub_features = ['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2']\n",
    "sub_train_data = train_data[sub_features]\n",
    "\n",
    "# Plot a pairgrid to study scatter plots \n",
    "g = sns.PairGrid(sub_train_data)\n",
    "g.map(sns.scatterplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "1. Since we would be using decison-tree based estimartors instead of linear models, computing correlation matrix isn't helpful \n",
    "2. Also, under the hood, the assumption behind evaluation of Pearrson and Spearmann's correlations is that the distribution of these features WRT to each other should be linear or monotonic\n",
    "3. As we can see from the above scatterplots, there isn't any linear or monotonic relationship between the features. Hence, we won't be using correlation values for feature selection\n",
    "4. Since we are using tree-based models, let's use ExtraTreesRegressor for feature selection. It differs from Random Forest in the way that it considers the entire dataset while botstrapping, whereas the Random Forest considers sampling with replacement. Also, the former is faster as it performs random split in the first instance, whereas the Random Forest computes the best split based on information gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T06:30:40.024305Z",
     "iopub.status.busy": "2022-06-14T06:30:40.023875Z",
     "iopub.status.idle": "2022-06-14T06:30:42.200310Z",
     "shell.execute_reply": "2022-06-14T06:30:42.199396Z",
     "shell.execute_reply.started": "2022-06-14T06:30:40.024272Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use Extra Trees Regressor for feature selection\n",
    "selector = SelectFromModel(estimator=ExtraTreesRegressor(criterion='mse', random_state=seed_value)).fit(train_data, y)\n",
    "mask = selector.get_support()\n",
    "feature_names = train_data.columns\n",
    "new_features = []\n",
    "\n",
    "for mask_val, feature in zip(mask, feature_names):\n",
    "    if mask_val == True:\n",
    "        new_features.append(feature)\n",
    "        \n",
    "train_data_1 = selector.fit_transform(train_data, y)\n",
    "test_data_1 = selector.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Future Checkboxes:\n",
    "<p>\n",
    "<input type=\"checkbox\"> Try KNN Imputer for imputation of categorical columns \n",
    "</p>\n",
    "\n",
    "<p> \n",
    "<input type=\"checkbox\"> Try Gradient Boosting algorithms for a better performance\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T07:06:12.481893Z",
     "iopub.status.busy": "2022-06-14T07:06:12.481475Z",
     "iopub.status.idle": "2022-06-14T07:06:12.497966Z",
     "shell.execute_reply": "2022-06-14T07:06:12.496847Z",
     "shell.execute_reply.started": "2022-06-14T07:06:12.481857Z"
    }
   },
   "outputs": [],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# Convert training and testing data to pandas DataFrame\n",
    "train_data_1 = pd.DataFrame(train_data_1)\n",
    "test_data_1 = pd.DataFrame(test_data_1)\n",
    "train_data_1.columns, test_data_1.columns = new_features, new_features\n",
    "\n",
    "# Split the dataset into training and validation sets (according to feature selection based on ExtraTreesRegressor, which is based upon random feature split)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_data_1, y, test_size=0.2, random_state=seed_value, shuffle=True)\n",
    "\n",
    "# Split the dataset into training and validation sets (without the feature selection by ExtraTreesRegressor)\n",
    "X_train_1, X_valid_1, y_train_1, y_valid_1 = train_test_split(train_data, y, test_size=0.2, random_state=seed_value, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T07:06:45.877358Z",
     "iopub.status.busy": "2022-06-14T07:06:45.876996Z",
     "iopub.status.idle": "2022-06-14T07:06:45.885205Z",
     "shell.execute_reply": "2022-06-14T07:06:45.884159Z",
     "shell.execute_reply.started": "2022-06-14T07:06:45.877327Z"
    }
   },
   "outputs": [],
   "source": [
    "# Impute missing values in training and validation datasets\n",
    "# X_train = pd.DataFrame(X_train)\n",
    "# X_valid = pd.DataFrame(X_valid)\n",
    "# X_train.columns = new_features\n",
    "# X_valid.columns = new_features\n",
    "X_train = X_train.fillna(0)\n",
    "X_valid = X_valid.fillna(0)\n",
    "\n",
    "X_train_1 = X_train_1.fillna(0)\n",
    "X_valid_1 = X_valid_1.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T07:06:56.049491Z",
     "iopub.status.busy": "2022-06-14T07:06:56.049091Z",
     "iopub.status.idle": "2022-06-14T07:06:56.056988Z",
     "shell.execute_reply": "2022-06-14T07:06:56.055647Z",
     "shell.execute_reply.started": "2022-06-14T07:06:56.049458Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate Root-Mean Squared Error\n",
    "def rmsle(y, preds):\n",
    "    return np.sqrt(mean_squared_error(y, preds))\n",
    "\n",
    "# Calculate R-2 and Adjusted R2\n",
    "def r2_value(dataframe, y, preds):\n",
    "    no_of_instances = len(dataframe)\n",
    "    no_of_features = len(dataframe.columns)\n",
    "    r2score = r2_score(y, preds)\n",
    "    adj_r2score = 1 - (((1-r2score)*(no_of_instances-1))/(no_of_instances-no_of_features-1))\n",
    "    return r2score, adj_r2score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Decision tree***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T06:58:58.019850Z",
     "iopub.status.busy": "2022-06-14T06:58:58.019405Z",
     "iopub.status.idle": "2022-06-14T06:58:58.048451Z",
     "shell.execute_reply": "2022-06-14T06:58:58.047498Z",
     "shell.execute_reply.started": "2022-06-14T06:58:58.019816Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the regressor \n",
    "dt_reg = DecisionTreeRegressor(random_state=seed_value, criterion='mse')\n",
    "dt_reg.fit(X_train, y_train)\n",
    "preds_dt = dt_reg.predict(X_valid)\n",
    "\n",
    "# Calculate rmse score \n",
    "rmse_dt = rmsle(y_valid, preds_dt)\n",
    "\n",
    "# Calculate R2 and Adjusted R2 score\n",
    "r2_dt, r_adj_dt = r2_value(train_data_1, y_valid, preds_dt)\n",
    "\n",
    "print(f\"The RMSE value for Random Forest Classifier is {rmse_dt}\")\n",
    "print(f\"The R2 value for Random Forest Classifier is {r2_dt}\")\n",
    "print(f\"The Adjusted R2 value for Random Forest Classifier is {r_adj_dt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Random Forest(Hyperparameter Tuning with RandomizedSearch CV + K-Fold Cross Validation)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T07:00:40.622471Z",
     "iopub.status.busy": "2022-06-14T07:00:40.622086Z",
     "iopub.status.idle": "2022-06-14T07:00:41.741587Z",
     "shell.execute_reply": "2022-06-14T07:00:41.740244Z",
     "shell.execute_reply.started": "2022-06-14T07:00:40.622440Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Random Forest Regressor\n",
    "random_reg = RandomForestRegressor(bootstrap=True, oob_score=True, criterion='mse', random_state=seed_value, verbose=2)\n",
    "random_reg.fit(X_train, y_train)\n",
    "preds = random_reg.predict(X_valid)\n",
    "\n",
    "# Calculate rmse score\n",
    "rmse_rf = rmsle(y_valid, preds)\n",
    "\n",
    "# Calculate R2 and Adjusted R2 scores\n",
    "r2_rf, adj_r2_rf = r2_value(train_data_1, y_valid, preds)\n",
    "\n",
    "\n",
    "print(f\"The RMSE value for Random Forest Regressor is {rmse_rf}\")\n",
    "print(f\"The R2 value for Random Forest Regressor is {r2_rf}\")\n",
    "print(f\"The Adjusted R2 value for Random Forest Regressor is {adj_r2_rf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T07:08:26.310392Z",
     "iopub.status.busy": "2022-06-14T07:08:26.309964Z",
     "iopub.status.idle": "2022-06-14T07:08:29.276365Z",
     "shell.execute_reply": "2022-06-14T07:08:29.275360Z",
     "shell.execute_reply.started": "2022-06-14T07:08:26.310358Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Random Forest Regressor\n",
    "random_reg_1 = RandomForestRegressor(bootstrap=True, oob_score=True, criterion='mse', random_state=seed_value, verbose=2)\n",
    "random_reg_1.fit(X_train_1, y_train_1)\n",
    "preds_1 = random_reg_1.predict(X_valid_1)\n",
    "\n",
    "# Calculate rmse score\n",
    "rmse_rf_1 = rmsle(y_valid_1, preds_1)\n",
    "\n",
    "# Calculate R2 and Adjusted R2 scores\n",
    "r2_rf_1, adj_r2_rf_1 = r2_value(train_data_1, y_valid_1, preds_1)\n",
    "\n",
    "\n",
    "print(f\"The RMSE value for Random Forest Regressor is {rmse_rf_1}\")\n",
    "print(f\"The R2 value for Random Forest Regressor is {r2_rf_1}\")\n",
    "print(f\"The Adjusted R2 value for Random Forest Regressor is {adj_r2_rf_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T07:08:44.653329Z",
     "iopub.status.busy": "2022-06-14T07:08:44.652975Z",
     "iopub.status.idle": "2022-06-14T07:08:44.659127Z",
     "shell.execute_reply": "2022-06-14T07:08:44.658029Z",
     "shell.execute_reply.started": "2022-06-14T07:08:44.653298Z"
    }
   },
   "outputs": [],
   "source": [
    "# Printing the estimator for this model \n",
    "print(random_reg_1.base_estimator_)\n",
    "\n",
    "# Print training set out-of-sample score\n",
    "print(random_reg_1.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T07:09:29.333799Z",
     "iopub.status.busy": "2022-06-14T07:09:29.333389Z",
     "iopub.status.idle": "2022-06-14T07:09:29.343580Z",
     "shell.execute_reply": "2022-06-14T07:09:29.342392Z",
     "shell.execute_reply.started": "2022-06-14T07:09:29.333762Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use GridSearchCV for best combination of hyperparameters and check if our model generalizes further\n",
    "# Use the entire dataset for training here as it would enable the model to learn from a larger pool of training instances \n",
    "\n",
    "# Initialize K-Fold Cross-Validation \n",
    "cv = KFold(shuffle=True, random_state=seed_value)\n",
    "\n",
    "params_random_reg = {\n",
    "    'n_estimators': [estim for estim in range(100, 500, 50)], \n",
    "    'max_depth': [depth for depth in range(10, 100, 5)], \n",
    "    'min_samples_leaf': [min_leaves for min_leaves in range(50, 100, 5)], \n",
    "    'max_samples': [min_samp for min_samp in range(100, 500, 50)], \n",
    "    'max_features': ['sqrt', 'log2', 'auto']\n",
    "}\n",
    "\n",
    "param_random_reg_2 = {\n",
    "                        'bootstrap': [True, False],\n",
    "                        'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "                        'max_features': ['auto', 'sqrt'],\n",
    "                        'min_samples_leaf': [1, 2, 4],\n",
    "                        'min_samples_split': [2, 5, 10],\n",
    "                        'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]\n",
    "}\n",
    "\n",
    "# random_model = GridSearchCV(random_reg, param_grid = params_random_reg, cv=cv)\n",
    "# random_model.fit(train_data, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T07:16:01.709427Z",
     "iopub.status.busy": "2022-06-14T07:16:01.709019Z",
     "iopub.status.idle": "2022-06-14T07:16:01.714429Z",
     "shell.execute_reply": "2022-06-14T07:16:01.713088Z",
     "shell.execute_reply.started": "2022-06-14T07:16:01.709389Z"
    }
   },
   "outputs": [],
   "source": [
    "# Grid Search CV is taking too much time\n",
    "# Reinitialize another random forest model \n",
    "# Let's use Randomized Search CV and use it to for estimating the range of values to be used in GridSearch CV \n",
    "# rf_reg_2 = RandomForestRegressor(random_state=seed_value)\n",
    "# random_model_2 = RandomizedSearchCV(rf_reg_2, n_iter=20, param_distributions = param_random_reg_2, cv=cv)\n",
    "# random_model_2.fit(train_data, y)\n",
    "# Get the best parameter values \n",
    "# print(random_model_2.best_params_)\n",
    "# print(random_model_2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T07:16:17.078045Z",
     "iopub.status.busy": "2022-06-14T07:16:17.077388Z",
     "iopub.status.idle": "2022-06-14T07:16:17.654276Z",
     "shell.execute_reply": "2022-06-14T07:16:17.653437Z",
     "shell.execute_reply.started": "2022-06-14T07:16:17.077994Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's estimate validation set on this range of hyperparameters\n",
    "final_random_model = RandomForestRegressor(n_estimators=300, min_samples_leaf=55, max_samples=400, max_features='log2', max_depth=45)\n",
    "final_random_model.fit(X_train, y_train)\n",
    "preds_val_3 = final_random_model.predict(X_valid)\n",
    "rmse_val_3 = rmsle(y_valid, preds_val_3)\n",
    "r2_val_3, adj2_val_3 = r2_value(train_data, y_valid, preds_val_3)\n",
    "\n",
    "print(f\"The RMSE value for tuned Random Forest Classifier is {rmse_val_3}\")\n",
    "print(f\"The R2 value for tuned Random Forest Classifier is {r2_val_3}\")\n",
    "print(f\"The Adjusted R2 value for Random Forest Classifier is {adj2_val_3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "1. After obtaining the best set of parameters through Randomized Search CV, the R2 and adjusted R2 value got reduced. Also the score for test set got reduced on submission. \n",
    "2. Maybe its is due to the fact that since the model selection tool has randomly initialized a set of parameters, without fitting each of the model to the dataset. The \n",
    "   resultant model hence, doesn't generalize well "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T07:16:49.809305Z",
     "iopub.status.busy": "2022-06-14T07:16:49.808557Z",
     "iopub.status.idle": "2022-06-14T07:16:49.835341Z",
     "shell.execute_reply": "2022-06-14T07:16:49.834297Z",
     "shell.execute_reply.started": "2022-06-14T07:16:49.809266Z"
    }
   },
   "outputs": [],
   "source": [
    "# Understanding feature importances of independent variables\n",
    "importance = random_reg_1.feature_importances_\n",
    "feat_importances = pd.Series(importance, index=train_data.columns)\n",
    "feat_importances.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "1. Except for some top 10-12 features, rest of them have a negligible impact on prediction of house prices. \n",
    "2. From some preliminary observations, rating of the overall material and finish of the house has a largest bearing on final house prices\n",
    "3. Rest of the features have a reduced impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T07:17:19.521845Z",
     "iopub.status.busy": "2022-06-14T07:17:19.521253Z",
     "iopub.status.idle": "2022-06-14T07:17:19.549271Z",
     "shell.execute_reply": "2022-06-14T07:17:19.548393Z",
     "shell.execute_reply.started": "2022-06-14T07:17:19.521792Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's verify these results with an F-test \n",
    "fs_imp_verify = pd.DataFrame()\n",
    "test = f_regression(X_train, y_train)\n",
    "fs_imp_verify['Feature'] = train_data_1.columns\n",
    "fs_imp_verify['Critical Value'] = test[0]\n",
    "fs_imp_verify['P-Value'] = test[1]\n",
    "fs_imp_verify.sort_values('P-Value')\n",
    "fs_imp_verify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "1. Here we have used the F-Test to verify the feature importance values provided by the random forest regressor \n",
    "2. The test verifies that MsSubClass is indeed the most important feature for calculation of house prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. By adding the additional training data, the overall R2 and adjusted R2 values have decreased, probably due to increase in number of training instances\n",
    "2. Performing data augmentation has enabled the model to generalize better, as evident from the improved test set score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTUETqNLuVPl"
   },
   "source": [
    "**Defining Cross-Validation and all possible regression evaluation metrics in the notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T09:45:51.466119Z",
     "iopub.status.busy": "2022-06-09T09:45:51.465681Z",
     "iopub.status.idle": "2022-06-09T09:45:51.472124Z",
     "shell.execute_reply": "2022-06-09T09:45:51.470829Z",
     "shell.execute_reply.started": "2022-06-09T09:45:51.466073Z"
    },
    "id": "Dsze3A-2v6UI"
   },
   "outputs": [],
   "source": [
    "# Define Cross Validation and other relatable metrics here. The cell has been deleted\n",
    "# ridge_regressor = Ridge() \n",
    "# params = {'alpha': [1,0.1,0.01,0.001,0.0001,0] , \"fit_intercept\": [True, False], \"solver\": ['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']}\n",
    "# scaler = RobustScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T09:45:51.474029Z",
     "iopub.status.busy": "2022-06-09T09:45:51.473596Z",
     "iopub.status.idle": "2022-06-09T09:45:51.488865Z",
     "shell.execute_reply": "2022-06-09T09:45:51.487792Z",
     "shell.execute_reply.started": "2022-06-09T09:45:51.473999Z"
    },
    "id": "JQNtspdB1l__",
    "outputId": "a95367e1-beb1-45d2-9696-d568cd8d8fad"
   },
   "outputs": [],
   "source": [
    "# grid_ridge = GridSearchCV(ridge_regressor, param_grid=params, cv=kfold, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "# grid_ridge.fit(X_train, y_train)\n",
    "# alpha = grid_ridge.best_params_\n",
    "# ridge_score = grid_ridge.best_score_\n",
    "# print(alpha)\n",
    "# print(ridge_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T09:45:51.490717Z",
     "iopub.status.busy": "2022-06-09T09:45:51.490248Z",
     "iopub.status.idle": "2022-06-09T09:45:51.501016Z",
     "shell.execute_reply": "2022-06-09T09:45:51.499911Z",
     "shell.execute_reply.started": "2022-06-09T09:45:51.490673Z"
    },
    "id": "K8IQn5543wZU"
   },
   "outputs": [],
   "source": [
    "# regressor_ridge_best = Ridge(alpha=0.01, fit_intercept='True', solver='cholesky')\n",
    "# regressor_ridge_best.fit(X_train, y_train)\n",
    "# predictions = regressor_ridge_best.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T09:45:51.50282Z",
     "iopub.status.busy": "2022-06-09T09:45:51.50228Z",
     "iopub.status.idle": "2022-06-09T09:45:51.513242Z",
     "shell.execute_reply": "2022-06-09T09:45:51.512346Z",
     "shell.execute_reply.started": "2022-06-09T09:45:51.502772Z"
    },
    "id": "70S6njvV4Me2",
    "outputId": "fd00bb1e-f1c6-497f-f377-4bfa39d49630"
   },
   "outputs": [],
   "source": [
    "# eval = rmsle(y_val, predictions)\n",
    "# r2_score(y_val, predictions) #defined as (1 - (SSres/SStot.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T09:45:51.514975Z",
     "iopub.status.busy": "2022-06-09T09:45:51.514568Z",
     "iopub.status.idle": "2022-06-09T09:45:51.52524Z",
     "shell.execute_reply": "2022-06-09T09:45:51.523789Z",
     "shell.execute_reply.started": "2022-06-09T09:45:51.514931Z"
    }
   },
   "outputs": [],
   "source": [
    "# regressor_ridge_best.fit(X, y)\n",
    "# predictions_final = regressor_ridge_best.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T07:17:46.990709Z",
     "iopub.status.busy": "2022-06-14T07:17:46.990340Z",
     "iopub.status.idle": "2022-06-14T07:17:46.995446Z",
     "shell.execute_reply": "2022-06-14T07:17:46.994337Z",
     "shell.execute_reply.started": "2022-06-14T07:17:46.990676Z"
    }
   },
   "outputs": [],
   "source": [
    "# Impute 'Missing' with '0' as there are only a few of these values\n",
    "# for index in test_data.index:\n",
    "#     test_data['BsmtFullBath'] = np.where(test_data['BsmtFullBath']=='Missing', 0, test_data['BsmtFullBath'])\n",
    "#     test_data['BsmtHalfBath'] = np.where(test_data['BsmtHalfBath']=='Missing', 0, test_data['BsmtHalfBath'])\n",
    "\n",
    "\n",
    "# Now cast the 'object' columns as 'float' types\n",
    "# convert_dtypes = {\n",
    "#                     'BsmtFullBath': 'float64',\n",
    "#                     'BsmtHalfBath': 'float64', \n",
    "#                     'BsmtFinSF1' : 'float64', \n",
    "#                     'BsmtFinSF2' : 'float64', \n",
    "#                     'BsmtUnfSF' : 'float64', \n",
    "#                     'TotalBsmtSF': 'float64', \n",
    "#                }\n",
    "# test_data = test_data.astype(convert_dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***We have got a pretty appreciable R2 and adjusted R2 score on the training dataset. Let's predict for the test dataset***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T10:03:45.746138Z",
     "iopub.status.busy": "2022-06-14T10:03:45.745359Z",
     "iopub.status.idle": "2022-06-14T10:03:45.792679Z",
     "shell.execute_reply": "2022-06-14T10:03:45.790740Z",
     "shell.execute_reply.started": "2022-06-14T10:03:45.745867Z"
    }
   },
   "outputs": [],
   "source": [
    "preds_final = random_reg_1.predict(test_data)\n",
    "\n",
    "# Submission of final predictions \n",
    "submission = pd.read_csv('../input/house-prices-advanced-regression-techniques/sample_submission.csv')\n",
    "submission.iloc[:, 1] = preds_final\n",
    "submission.to_csv(\"./submission_prediction.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retraining model with reduced number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
