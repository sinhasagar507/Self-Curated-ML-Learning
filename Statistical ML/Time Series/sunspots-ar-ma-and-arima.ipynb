{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np #linear algebra\nimport pandas as pd #data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #data visualization\nimport seaborn as sns  #data visualization\nfrom scipy import stats #stats library\nfrom pylab import rcParams\n\n\n#Matplotlib runtime(rc) configuration options\nrcParams['figure.figsize'] = 11, 9\nsns.set_theme(style = \"whitegrid\")\n\n\n\n#Coerce warning issues\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\n# Importing time-based libraries\nimport time\nfrom datetime import time\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n        \n\n# Libraries for statistical visualization in time-series\nfrom pandas.plotting import autocorrelation_plot as ap, lag_plot\nfrom scipy.stats import boxcox\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.ar_model import AutoReg, AR\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n\nfrom sklearn.metrics import mean_squared_error\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-12T10:20:47.580636Z","iopub.execute_input":"2022-07-12T10:20:47.581439Z","iopub.status.idle":"2022-07-12T10:20:49.379034Z","shell.execute_reply.started":"2022-07-12T10:20:47.581383Z","shell.execute_reply":"2022-07-12T10:20:49.377938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Suggestions and tips for a forecasting model:\n\n**Summary of 5 step Iterative process and Iterative Forecast Development Process**\n* Select or devise a time series forecast process that is tailored to your project, tools, team,\nand level of expertise.\n*  Write down all assumptions and questions you have during analysis and forecasting work,\nthen revisit them later and seek to answer them with small experiments on historical data.\n* Review a large number of plots of your data at different time scales, zooms, and transforms\nof observations in an effort to help make exploitable structures present in the data obvious\nto you.\n* Develop a robust test harness for evaluating models using a meaningful performance\nmeasure and a reliable test strategy, such as walk-forward validation (rolling forecast).\n* Start with simple naive forecast models to provide a baseline of performance for more\nsophisticated methods to improve upon.\n* Create a large number of perspectives or views on your time series data, including a suite\nof automated transforms, and evaluate each with one or a suite of models in order to help\nautomatically discover non-intuitive representations and model combinations that result\nin good predictions for your problem.\n* Try a suite of models of differing types on your problem, from simple to more advanced\napproaches.\n* Try a suite of configurations for a given problem, including configurations that have worked\nwell on other problems.\n* Try automated hyperparameter optimization methods for models to flush out a suite of\nwell-performing models as well as non-intuitive model configurations that you would not\nhave tried manually.\n* Devise automated tests of performance and skill for ongoing predictions to help to\nautomatically determine if and when a model has become stale and requires review or\nretraining.","metadata":{}},{"cell_type":"markdown","source":"# Data Description and Task\nSunspots are temporary phenomena on the Sun's photosphere that appear as spots darker than the surrounding areas. They are regions of reduced surface temperature caused by concentrations of magnetic field flux that inhibit convection. Sunspots usually appear in pairs of opposite magnetic polarity. Their number varies according to the approximately 11-year solar cycle.\n\nInformation : https://en.wikipedia.org/wiki/Sunspot\nSource: https://www.sidc.be/silso/INFO/snmtotcsv.php\n\nThe dataset has been sourced from SIDC website. It seems that the data is captured on a daily basis, and is provided in four formats, as DAILY TOTAL SUNSPOT NUMBER, MONTHLY MEAN SUNSPOT NUMBER, 13-MONTH SMOOTHED MONTHLY TOTAL SUNSPOT NUMBER and YEARLY MEAN TOTAL SUNSPOT NUMBER. All four datasets have different starting periods and the observations have been recorded till the current date. For my analysis, I have chosen the dataset on MONTHLY MEAN SUNSPOT NUMBER \n\n\nData Description:\nADD LATER \n* Task: Predict the requisite number of sunspots for the upcoming month.\n","metadata":{}},{"cell_type":"markdown","source":"TO DO LIST\n- [x] Data Cleaning\n   - [x] Erroneous data types and value encoding \n- [x] Basic Statistical Analysis\n   - [x] Null Values \n   - [x] Duplicates and data types \n   - [x] Description and information \n- [x] Date Time Based Analysis\n   - [x] Origin of Timestamps - How are they generated?\n   - [x] Time Interval - Regular or Ireegular\n   - [x] Extracting Year, Month, Date and Day \n- [x] Preliminary Visualization \n   - [x] Analyze univariate plots(scatter plot, lag plots, histograms, KDE and boxplots) \n   - [x] Heatmpas and Autcorrelation plots \n   - [ ] Analyze the data for different periods - upsampling or downsampling data - DOMAIN SPECIFIC question - ask that to yourself - THIS PART WILL ARRIVE IN DETAILED ANALYSIS section\n   - [ ] Check for a distorted distribution after the POWER Transform\n   - [ ] Answering why do I want to capture the next month's data rather than the yearly data \n   - [ ] Resampling data and capturing rolling mean statistics \n   - [ ] Predict yearly occurences of average no. of sunspots \n   - [ ] Group all monthly data and capture several statistics - all plots - analyze again \n   - [ ] First group the data and analyze, then perform resampling and derive all statistical inferences ","metadata":{}},{"cell_type":"markdown","source":"## LOADING DATA AND DEVELOPMENT OF TEST HARNESS","metadata":{}},{"cell_type":"code","source":"__author__ = \"Sagar Sinha\"\n__NOTEBOOK__ = \"Univariate Time Series Prediction\"","metadata":{"execution":{"iopub.status.busy":"2022-07-12T10:20:57.807235Z","iopub.execute_input":"2022-07-12T10:20:57.807925Z","iopub.status.idle":"2022-07-12T10:20:57.812040Z","shell.execute_reply.started":"2022-07-12T10:20:57.807885Z","shell.execute_reply":"2022-07-12T10:20:57.811110Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"../input/sunspots/Sunspots.csv\", parse_dates=True, infer_datetime_format=True, skip_blank_lines=True) # Reading the dataset\ncurr_data = pd.read_csv(\"../input/sunspot-no-till-current-date/SN_m_tot_V2.0.csv\", parse_dates=True, infer_datetime_format=True, skip_blank_lines=True)\ndata.drop(['Unnamed: 0'], axis=1, inplace=True)\ndata.columns = ['date', 'no_of_sunspots']\ndata.drop(3264, axis=0, inplace=True) # Dropping the last row as there exists only a single value with that year value. It would introduce certain issues later on ","metadata":{"execution":{"iopub.status.busy":"2022-07-12T10:20:59.578452Z","iopub.execute_input":"2022-07-12T10:20:59.579171Z","iopub.status.idle":"2022-07-12T10:20:59.637002Z","shell.execute_reply.started":"2022-07-12T10:20:59.579132Z","shell.execute_reply":"2022-07-12T10:20:59.636185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"curr_data.columns = curr_data.columns.str.split(\";\")\ncurr_data.columns = [\"Combined_Data\"]\ncurr_data[[\"Year\", \"Month\", \"Frac\", \"Sunspots\", \"NR_1\", \"NR_2\", \"NR_3\"]] = curr_data[\"Combined_Data\"].str.split(\";\", expand = True)\ncurr_data.drop([\"Combined_Data\", \"Frac\", \"NR_1\", \"NR_2\", \"NR_3\"], axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T10:21:02.806412Z","iopub.execute_input":"2022-07-12T10:21:02.807143Z","iopub.status.idle":"2022-07-12T10:21:02.836340Z","shell.execute_reply.started":"2022-07-12T10:21:02.807104Z","shell.execute_reply":"2022-07-12T10:21:02.835093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### When we would require it, format the datetime column \n### We only need the current data from February 2021 onwards. This is the dataset for prediction \ncurr_data_req = curr_data[3263: ]\n### Rename column of current data\ncurr_data_req = curr_data_req.rename(columns = {\"Sunspots\": \"no_of_sunspots\"})","metadata":{"execution":{"iopub.status.busy":"2022-07-12T10:21:08.337865Z","iopub.execute_input":"2022-07-12T10:21:08.338567Z","iopub.status.idle":"2022-07-12T10:21:08.343846Z","shell.execute_reply.started":"2022-07-12T10:21:08.338526Z","shell.execute_reply":"2022-07-12T10:21:08.342715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data.isnull().sum())  # Check for null values\nprint(\"\\n\")\nprint(curr_data_req.isnull().sum())\nprint(\"\\n\")\nprint(data.dtypes)\nprint(\"\\n\")\nprint(curr_data_req.dtypes)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T10:21:10.440702Z","iopub.execute_input":"2022-07-12T10:21:10.441431Z","iopub.status.idle":"2022-07-12T10:21:10.456358Z","shell.execute_reply.started":"2022-07-12T10:21:10.441388Z","shell.execute_reply":"2022-07-12T10:21:10.455337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The 'Date' column is of 'object' data type. It needs to be converted into the datetime format**","metadata":{}},{"cell_type":"code","source":"data['date'] = data['date'].astype('datetime64') # Type-casting ['date'] column to Pandas datetime format\ndata = data.drop_duplicates() # Removes all duplicate rows, if any ","metadata":{"execution":{"iopub.status.busy":"2022-07-12T10:21:15.303414Z","iopub.execute_input":"2022-07-12T10:21:15.303836Z","iopub.status.idle":"2022-07-12T10:21:15.312699Z","shell.execute_reply.started":"2022-07-12T10:21:15.303796Z","shell.execute_reply":"2022-07-12T10:21:15.311401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Convert data into a series\ndata.set_index(['date'], inplace=True) # Setting 'date' as the index column \ndata.index.name = None\nsunspots = data['no_of_sunspots']","metadata":{"execution":{"iopub.status.busy":"2022-07-12T10:21:17.208771Z","iopub.execute_input":"2022-07-12T10:21:17.209310Z","iopub.status.idle":"2022-07-12T10:21:17.215522Z","shell.execute_reply.started":"2022-07-12T10:21:17.209277Z","shell.execute_reply":"2022-07-12T10:21:17.214526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data.describe())\nprint()\nprint(data.info())","metadata":{"execution":{"iopub.status.busy":"2022-07-12T10:21:19.792361Z","iopub.execute_input":"2022-07-12T10:21:19.792774Z","iopub.status.idle":"2022-07-12T10:21:19.818885Z","shell.execute_reply.started":"2022-07-12T10:21:19.792737Z","shell.execute_reply":"2022-07-12T10:21:19.817708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### We have the training and test datasets. Let's create the validation data set - We would split the dataset as: TRAINING=0.7%, VALIDATION=0.2%, TEST=0.1%\n#### No. of training samples = 3234\n#### No. of validation samples = 34\n#### No. of test samples = 17\ntrain_data = data[0:3240]\nval_data = data[3240: ]\ntest_data = curr_data_req","metadata":{"execution":{"iopub.status.busy":"2022-07-12T10:21:22.623992Z","iopub.execute_input":"2022-07-12T10:21:22.624721Z","iopub.status.idle":"2022-07-12T10:21:22.631444Z","shell.execute_reply.started":"2022-07-12T10:21:22.624667Z","shell.execute_reply":"2022-07-12T10:21:22.630507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Choice of evaluation metric = RMSE. Since we want to count of sunspots\n### Development of persistence model on validation data\nval_data[\"no_of_sunspots_lagged\"] = val_data[\"no_of_sunspots\"].shift(1)\n\n### Backfilling row data consisting of NaN values\nval_data_persist = val_data.bfill(axis = \"rows\")\n\n#### Store predictions\ntruth_vals = np.array([value for value in val_data_persist[\"no_of_sunspots\"]])\npred_vals = np.array([value for value in val_data_persist[\"no_of_sunspots_lagged\"]])\n\n#### Calculate RMSE and plot the results \nrmse = np.sqrt(mean_squared_error(truth_vals, pred_vals))\n\n#### Plotting the results   \nfig, (ax1, ax2) = plt.subplots(2, 1)\nline1, = ax1.plot(truth_vals[0:500], color = \"r\", linestyle = \"-\", alpha = 0.8)\nline2, = ax2.plot(pred_vals[0:500], color = \"y\", linestyle= \"-\", alpha = 0.8)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T10:21:25.031923Z","iopub.execute_input":"2022-07-12T10:21:25.032841Z","iopub.status.idle":"2022-07-12T10:21:25.453773Z","shell.execute_reply.started":"2022-07-12T10:21:25.032786Z","shell.execute_reply":"2022-07-12T10:21:25.452807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(rmse)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T10:21:29.118347Z","iopub.execute_input":"2022-07-12T10:21:29.119140Z","iopub.status.idle":"2022-07-12T10:21:29.125248Z","shell.execute_reply.started":"2022-07-12T10:21:29.119075Z","shell.execute_reply":"2022-07-12T10:21:29.124209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Create a lagged variable for the validation dataset \nval_data[\"no_of_sunspots_lagged\"] = val_data[\"no_of_sunspots\"].shift(1)\nval_data = val_data.bfill(axis = \"rows\")","metadata":{"execution":{"iopub.status.busy":"2022-07-12T10:21:30.851033Z","iopub.execute_input":"2022-07-12T10:21:30.851451Z","iopub.status.idle":"2022-07-12T10:21:30.857372Z","shell.execute_reply.started":"2022-07-12T10:21:30.851414Z","shell.execute_reply":"2022-07-12T10:21:30.856475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observations\nThe baseline model is pretty close to the original one, and it serves us a good starting point \n* The baseline RMSE value is 6.576","metadata":{}},{"cell_type":"markdown","source":"## PRELIMINARY VISUALIZATIONS AND OBSERVATIONS","metadata":{}},{"cell_type":"code","source":"## Determine the time interval and frequency of occurence of sunspots \n## The underlying assumption is that the data has a monthly frequency \n## Plot the entire series\n## Group the data at different scales and then plot the series\ntrain_data.plot()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T10:21:33.878501Z","iopub.execute_input":"2022-07-12T10:21:33.879282Z","iopub.status.idle":"2022-07-12T10:21:34.341387Z","shell.execute_reply.started":"2022-07-12T10:21:33.879225Z","shell.execute_reply":"2022-07-12T10:21:34.340477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Reset the indexes \n# train_data = train_data.set_index(\"date\")\n# val_data = val_data.set_index(\"date\")","metadata":{"execution":{"iopub.status.busy":"2022-07-12T08:43:09.349063Z","iopub.execute_input":"2022-07-12T08:43:09.349723Z","iopub.status.idle":"2022-07-12T08:43:09.354878Z","shell.execute_reply.started":"2022-07-12T08:43:09.349679Z","shell.execute_reply":"2022-07-12T08:43:09.353714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Let's plot yearly data, i.e., say for 1749 \nyear_data = train_data[\"1749-01-31\": \"1749-12-31\"]\nyear_data.plot()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T10:21:38.826280Z","iopub.execute_input":"2022-07-12T10:21:38.826898Z","iopub.status.idle":"2022-07-12T10:21:39.120476Z","shell.execute_reply.started":"2022-07-12T10:21:38.826860Z","shell.execute_reply":"2022-07-12T10:21:39.119478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[\"1749-1-31\": \"1760-1-31\"].min()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T10:21:46.794540Z","iopub.execute_input":"2022-07-12T10:21:46.795255Z","iopub.status.idle":"2022-07-12T10:21:46.807468Z","shell.execute_reply.started":"2022-07-12T10:21:46.795205Z","shell.execute_reply":"2022-07-12T10:21:46.806332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## According to an observation regarding sunspots, they follow an 11 month cycle, which is called the solar cycle \n## Caqpture 11 years from 1769 to 1780 \nfig, ax = plt.subplots(1, figsize=(20, 10))\nyear_range_sunspots_1 = train_data[\"1749-1-31\": \"1760-1-31\"]\nax.plot(year_range_sunspots_1, color=\"red\")\nax.axhline(y = year_range_sunspots_1.values.min(), linestyle = \"-\", color=\"g\")\nax.axhline(y = year_range_sunspots_1.values.max(), linestyle = \"-\", color=\"y\")\nax.text(x = year_range_sunspots_1.idxmax(), y = year_range_sunspots_1.values.max(), s=\"MAX\", horizontalalignment=\"left\", verticalalignment=\"top\", fontsize=20, color=\"blue\")\nax.text(x = year_range_sunspots_1.idxmin(), y = year_range_sunspots_1.values.min(), s=\"MIN\", horizontalalignment=\"right\", verticalalignment=\"bottom\", fontsize=20, color=\"blue\")","metadata":{"execution":{"iopub.status.busy":"2022-07-12T08:46:13.215399Z","iopub.execute_input":"2022-07-12T08:46:13.216226Z","iopub.status.idle":"2022-07-12T08:46:13.502097Z","shell.execute_reply.started":"2022-07-12T08:46:13.216169Z","shell.execute_reply":"2022-07-12T08:46:13.500500Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## According to an observation regarding sunspots, they follow an 11 year cycle, which is called the solar cycle \n## Capture 11 years from 1769 to 1780 \nplt.figure(figsize=(20, 10))\nyear_range_sunspots_1 = train_data[\"1749-1-31\": \"1760-1-31\"]\nyear_range_sunspots_2 = train_data[\"1760-2-1\": \"1772-1-31\"]\nyear_range_sunspots_1.plot()\nyear_range_sunspots_2.plot()\nplt.xlabel(\"Variation of average no. of sunspots over 11 year period\")\nplt.ylabel(\"No. of sunspots\")","metadata":{"execution":{"iopub.status.busy":"2022-07-12T08:46:22.430257Z","iopub.execute_input":"2022-07-12T08:46:22.430661Z","iopub.status.idle":"2022-07-12T08:46:23.005969Z","shell.execute_reply.started":"2022-07-12T08:46:22.430605Z","shell.execute_reply":"2022-07-12T08:46:23.004915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observations\n* It is interesting to note that the series has a stepwise fall in frequency of sunspots - prevalece of a large no. of sunspots has a lower frequency\n* Hence, there occurs only a few instances when the no. of sunspots are large in number \n* The series has an underlying cyclic feature. It is an indicative of the probable prevalence of seasonality component within data \n* Furthermore, we can observe that there is a maxima and a minima in an yearly period as well as in an 11 year period - the latter is known as solar cycle\n* Every 11 years, the no. of sunspots reach a maximum limit suddenly, and then gradually decreases to a minimum limit, rises suddenly and the cycle follows","metadata":{}},{"cell_type":"code","source":"# Group monthly data into yearly period\n## Convert sunspots to DataFrame \n## Drop rows for the year 2018 from training data \nannual_groups = train_data[\"no_of_sunspots\"].groupby(pd.Grouper(freq='A'))\nyear_group_data = {}\n\nfor index, group in annual_groups:\n    year_group_data[index.year] = group.values\n    \nyear_group_df = pd.DataFrame(year_group_data)\nyear_group_df = year_group_df.T","metadata":{"execution":{"iopub.status.busy":"2022-07-12T10:22:20.471949Z","iopub.execute_input":"2022-07-12T10:22:20.472563Z","iopub.status.idle":"2022-07-12T10:22:20.533635Z","shell.execute_reply.started":"2022-07-12T10:22:20.472530Z","shell.execute_reply":"2022-07-12T10:22:20.532432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"year_group_df[\"avg_over_year\"] = year_group_df.sum(axis = 1)/12\nyear_group_df","metadata":{"execution":{"iopub.status.busy":"2022-07-12T10:22:23.481129Z","iopub.execute_input":"2022-07-12T10:22:23.481521Z","iopub.status.idle":"2022-07-12T10:22:23.535769Z","shell.execute_reply.started":"2022-07-12T10:22:23.481479Z","shell.execute_reply":"2022-07-12T10:22:23.534691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"year_group_data","metadata":{}},{"cell_type":"code","source":"## Average number of sunspot values over the entire period\navg_by_year = {}\nfor year in year_group_df.columns:\n    avg_by_year[year] = year_group_df[year].mean()\n\nplt.plot(avg_year.values)\n\n# Plotting average by mean gives us a smoother result which can help us in predicting future yearly average values of solar spots ","metadata":{"execution":{"iopub.status.busy":"2022-07-12T10:24:41.726801Z","iopub.execute_input":"2022-07-12T10:24:41.727201Z","iopub.status.idle":"2022-07-12T10:24:41.949828Z","shell.execute_reply.started":"2022-07-12T10:24:41.727170Z","shell.execute_reply":"2022-07-12T10:24:41.948699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"avg_by_year.values()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T10:24:02.112462Z","iopub.execute_input":"2022-07-12T10:24:02.113155Z","iopub.status.idle":"2022-07-12T10:24:02.120291Z","shell.execute_reply.started":"2022-07-12T10:24:02.113102Z","shell.execute_reply":"2022-07-12T10:24:02.119196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Plotting autocorrelation plot\nap(avg_year)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T10:25:52.685774Z","iopub.execute_input":"2022-07-12T10:25:52.686215Z","iopub.status.idle":"2022-07-12T10:25:52.922657Z","shell.execute_reply.started":"2022-07-12T10:25:52.686166Z","shell.execute_reply":"2022-07-12T10:25:52.921325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ap(train_data)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T10:25:57.299369Z","iopub.execute_input":"2022-07-12T10:25:57.300160Z","iopub.status.idle":"2022-07-12T10:25:57.593106Z","shell.execute_reply.started":"2022-07-12T10:25:57.300120Z","shell.execute_reply":"2022-07-12T10:25:57.591871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplot(121)\nlag_plot(avg_year)\n\n### Print lag plot of the original dataset \nplt.subplot(122)\nlag_plot(train_data)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T10:26:43.061701Z","iopub.execute_input":"2022-07-12T10:26:43.062108Z","iopub.status.idle":"2022-07-12T10:26:43.493440Z","shell.execute_reply.started":"2022-07-12T10:26:43.062074Z","shell.execute_reply":"2022-07-12T10:26:43.492273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### set the spacing between subplots\nplt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=0.9, \n                    top=0.9, \n                    wspace=0.4, \n                    hspace=0.4)\n\n### The statistical relationship between avg spot no. of a particular month to a preceding month seems highly significant. Let's plot for multiple lags, say for upto 9 \ni, j, k = 3, 3, 1\n\nfor i in range(0, 9):\n    plt.subplot(330 + k)\n    lag_plot(train_data.shift(k))\n    plt.xlabel(\"t\")\n    plt.ylabel(\"y(t-\"+str(k)+\")\")\n    k += 1       ","metadata":{"execution":{"iopub.status.busy":"2022-07-12T10:26:45.946153Z","iopub.execute_input":"2022-07-12T10:26:45.946510Z","iopub.status.idle":"2022-07-12T10:26:47.457002Z","shell.execute_reply.started":"2022-07-12T10:26:45.946480Z","shell.execute_reply":"2022-07-12T10:26:47.456085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Analyze a boxplot for the same \nplt.boxplot(train_data)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T10:26:48.848513Z","iopub.execute_input":"2022-07-12T10:26:48.848902Z","iopub.status.idle":"2022-07-12T10:26:49.008987Z","shell.execute_reply.started":"2022-07-12T10:26:48.848868Z","shell.execute_reply":"2022-07-12T10:26:49.007842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### KDE plot of original data\ntrain_data.plot(kind = \"kde\")","metadata":{"execution":{"iopub.status.busy":"2022-07-12T10:26:52.279620Z","iopub.execute_input":"2022-07-12T10:26:52.280086Z","iopub.status.idle":"2022-07-12T10:26:52.651455Z","shell.execute_reply.started":"2022-07-12T10:26:52.280027Z","shell.execute_reply":"2022-07-12T10:26:52.650218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Histogram plot of original data\ntrain_data.plot(kind = \"hist\")","metadata":{"execution":{"iopub.status.busy":"2022-07-12T10:26:55.245887Z","iopub.execute_input":"2022-07-12T10:26:55.246313Z","iopub.status.idle":"2022-07-12T10:26:55.519557Z","shell.execute_reply.started":"2022-07-12T10:26:55.246277Z","shell.execute_reply":"2022-07-12T10:26:55.518716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.DataFrame(train_data)\ntrain_data.columns = [\"Sunspots\"]","metadata":{"execution":{"iopub.status.busy":"2022-07-12T10:27:00.831703Z","iopub.execute_input":"2022-07-12T10:27:00.832348Z","iopub.status.idle":"2022-07-12T10:27:00.836800Z","shell.execute_reply.started":"2022-07-12T10:27:00.832310Z","shell.execute_reply":"2022-07-12T10:27:00.836005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observations\n* We have capped and removed the outliers, but the distribution has been distorted. Use Power Transforms later","metadata":{}},{"cell_type":"markdown","source":"## Time Series decomposition ","metadata":{}},{"cell_type":"code","source":"### Decompose the dataframe into its required constituents \nsunspots_decomposed = seasonal_decompose(train_data[\"no_of_sunspots\"], model = \"additive\", freq=30 )","metadata":{"execution":{"iopub.status.busy":"2022-07-12T11:58:35.051007Z","iopub.execute_input":"2022-07-12T11:58:35.051399Z","iopub.status.idle":"2022-07-12T11:58:35.061339Z","shell.execute_reply.started":"2022-07-12T11:58:35.051365Z","shell.execute_reply":"2022-07-12T11:58:35.060439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Plotting decomposition \nsunspots_decomposed.plot()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T11:58:37.518746Z","iopub.execute_input":"2022-07-12T11:58:37.519389Z","iopub.status.idle":"2022-07-12T11:58:39.603312Z","shell.execute_reply.started":"2022-07-12T11:58:37.519337Z","shell.execute_reply":"2022-07-12T11:58:39.601985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observations\n* We can notice that there exists a periodic change in correlation from positive to negative in avg. no. of sunspots, with highly appreciable correlation values, for around 500 days\n* It coudl be that we need to downsample monthly data to an yearly one for incorportating this observation to the fullest. But for understanding purposes, let's model both, i.e., for yearly as well as diurnal data \n* The dataset has a strong trend as well as seasonal component \n* There are some occasional aberrations in the trend component between 1800-1840 and at some other places as well\n* There is a uniform seasonal component that can be removed. Removing the seasonal component may aid in smoothening the data  \n* The residual values seem to be uniformly distributed with constant variance, which is an indication that it doesn't have any time series component and doesn't contribute in forecasting. Hence, it can be directly removed. ","metadata":{}},{"cell_type":"code","source":"### Exploring the residual data \ntrain_data[\"resid\"] = sunspots_decomposed.resid\n\n### Impute NaN values in residuals with 0, as 0 is indicative of the absence of residual values \ntrain_data[\"resid\"] = train_data[\"resid\"].fillna(0)\n\n### Exploring seasonal data\ntrain_data[\"seasonal\"] = sunspots_decomposed.seasonal\n\n###  Impute NaN values in seasonal with 0, as 0 is indicative of the absence of seasonality \ntrain_data[\"seasonal\"] = train_data[\"seasonal\"].fillna(0)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T12:28:12.859724Z","iopub.execute_input":"2022-07-12T12:28:12.860255Z","iopub.status.idle":"2022-07-12T12:28:12.869531Z","shell.execute_reply.started":"2022-07-12T12:28:12.860202Z","shell.execute_reply":"2022-07-12T12:28:12.868472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Removing resduals \ntrain_data[\"de_resid\"] = train_data[\"no_of_sunspots\"] - train_data[\"resid\"]\n\n### Removing seasonality \ntrain_data[\"de_seasonal\"] = train_data[\"de_resid\"] - train_data[\"seasonal\"]","metadata":{"execution":{"iopub.status.busy":"2022-07-12T12:28:39.884579Z","iopub.execute_input":"2022-07-12T12:28:39.885153Z","iopub.status.idle":"2022-07-12T12:28:39.893318Z","shell.execute_reply.started":"2022-07-12T12:28:39.885102Z","shell.execute_reply":"2022-07-12T12:28:39.892175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Plotting the transformed data \ntrain_data[\"de_seasonal\"].plot(kind = \"kde\")","metadata":{"execution":{"iopub.status.busy":"2022-07-12T12:30:18.830046Z","iopub.execute_input":"2022-07-12T12:30:18.831073Z","iopub.status.idle":"2022-07-12T12:30:19.127801Z","shell.execute_reply.started":"2022-07-12T12:30:18.830998Z","shell.execute_reply":"2022-07-12T12:30:19.127002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Cap values beyond some positive and negative values and re-draw the curve \nupper_limit = 1.5 * np.mean(train_data[\"Sunspots\"])\ntrain_data[\"Sunspots\"] = np.where(train_data[\"Sunspots\"] > upper_limit, upper_limit, train_data[\"Sunspots\"])\n# train_data[\"Sunspots\"] = np.where(train_data[\"Sunspots\"] < 200, 200, train_data[\"Sunspots\"])\n\nsunspots.plot(kind=\"box\")\n# sunspots.plot(kind=\"kde\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Checking Stationarity of Time Series","metadata":{}},{"cell_type":"code","source":"test_results = adfuller(train_data[\"de_seasonal\"])\ntest_results","metadata":{"execution":{"iopub.status.busy":"2022-07-12T12:46:55.131011Z","iopub.execute_input":"2022-07-12T12:46:55.131724Z","iopub.status.idle":"2022-07-12T12:46:55.378434Z","shell.execute_reply.started":"2022-07-12T12:46:55.131673Z","shell.execute_reply":"2022-07-12T12:46:55.377266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Printing the test statistics\nprint(\"The critical value is %0.5f\" % (test_results[0]))\nprint(\"The p-value is %0.5f\" % (test_results[1]))","metadata":{"execution":{"iopub.status.busy":"2022-07-12T12:47:19.561346Z","iopub.execute_input":"2022-07-12T12:47:19.562033Z","iopub.status.idle":"2022-07-12T12:47:19.567871Z","shell.execute_reply.started":"2022-07-12T12:47:19.561997Z","shell.execute_reply":"2022-07-12T12:47:19.566769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observation: \n* As we can observe, the test-statistic is quite less than Z-value at p=0.01 or p=0.05. Hence we fail to accept the null hypothesis and declare that the series is stationary \n* No transformation or differencing is required ","metadata":{}},{"cell_type":"markdown","source":"## Time Series Modelling ","metadata":{}},{"cell_type":"code","source":"### Checking for AR and MA \n#### Since the model had no residuals, as observed during seasonal decomposition, it is clearly not an MA model \n#### Let's check for AR instead\nplot_pacf(train_data[\"de_seasonal\"])","metadata":{"execution":{"iopub.status.busy":"2022-07-12T12:50:30.454795Z","iopub.execute_input":"2022-07-12T12:50:30.455218Z","iopub.status.idle":"2022-07-12T12:50:30.922999Z","shell.execute_reply.started":"2022-07-12T12:50:30.455188Z","shell.execute_reply":"2022-07-12T12:50:30.922189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_acf(train_data[\"de_seasonal\"])","metadata":{"execution":{"iopub.status.busy":"2022-07-12T12:51:12.702903Z","iopub.execute_input":"2022-07-12T12:51:12.703479Z","iopub.status.idle":"2022-07-12T12:51:13.148474Z","shell.execute_reply.started":"2022-07-12T12:51:12.703446Z","shell.execute_reply":"2022-07-12T12:51:13.147614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observations:\n* As observed, the PACF plot shows a sharp decline in correlation, with an acceptable lag value of 25-28, whereas the ACF correlation values decline gradually over a longer period of time. Hence we will forecast the data on AR model with a lag value(p) of around 25-28 for our model ","metadata":{}},{"cell_type":"code","source":"### AR model\ntrain_values = train_data[\"de_seasonal\"]\nval_values = val_data[\"no_of_sunspots\"]\nhistory = [x for x in train_values]","metadata":{"execution":{"iopub.status.busy":"2022-07-12T13:00:54.119122Z","iopub.execute_input":"2022-07-12T13:00:54.119469Z","iopub.status.idle":"2022-07-12T13:00:54.125959Z","shell.execute_reply.started":"2022-07-12T13:00:54.119442Z","shell.execute_reply":"2022-07-12T13:00:54.124923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = AutoReg(history, lags = 28).fit()    \npreds = model.predict(start = len(train_values) + 1, end = len(train_values) + len(val_values), dynamic = False)\n\n### Some prediction values are negative. But negative values are unacceptable as sunspot numbers can't be negative \npreds_final = []\nfor pred in preds:\n    if pred < 0:\n        preds_final.append(0)\n    else:\n        preds_final.append(pred)\nprint(np.sqrt(mean_squared_error(val_data[\"no_of_sunspots\"].values, preds_final)))","metadata":{"execution":{"iopub.status.busy":"2022-07-12T13:09:22.718459Z","iopub.execute_input":"2022-07-12T13:09:22.719128Z","iopub.status.idle":"2022-07-12T13:09:22.749069Z","shell.execute_reply.started":"2022-07-12T13:09:22.719090Z","shell.execute_reply":"2022-07-12T13:09:22.748078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observations:\n* The RMSE for validation data with AR model is comaprable with the baseline model. We can still improve the model via - walk forward cross validation ","metadata":{}},{"cell_type":"code","source":"for pred in preds:\n    history.append(pred)\ntest_data[\"no_of_sunspots\"] = test_data[\"no_of_sunspots\"].astype(float)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T13:11:59.031282Z","iopub.execute_input":"2022-07-12T13:11:59.031680Z","iopub.status.idle":"2022-07-12T13:11:59.037953Z","shell.execute_reply.started":"2022-07-12T13:11:59.031647Z","shell.execute_reply":"2022-07-12T13:11:59.036624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = AutoReg(history, lags = 128).fit()\npreds_test = model.predict(start = len(train_values) + len(val_values) + 1, end = len(train_values) + len(val_values) + len(test_data), dynamic = False)\npred_test_final = []\nfor pred in preds_test:\n    \nprint(np.sqrt(mean_squared_error(test_data[\"no_of_sunspots\"].values, preds_test)))","metadata":{"execution":{"iopub.status.busy":"2022-07-12T13:12:02.442122Z","iopub.execute_input":"2022-07-12T13:12:02.442470Z","iopub.status.idle":"2022-07-12T13:12:02.742380Z","shell.execute_reply.started":"2022-07-12T13:12:02.442440Z","shell.execute_reply":"2022-07-12T13:12:02.740979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(test_data[\"no_of_sunspots\"].values)\nplt.plot(preds_test)\n\n# Function to add a legend  \nplt.legend([\"Original Test Observations\", \"Test Predictions\"], loc =\"lower right\")","metadata":{"execution":{"iopub.status.busy":"2022-07-12T13:13:13.418543Z","iopub.execute_input":"2022-07-12T13:13:13.419103Z","iopub.status.idle":"2022-07-12T13:13:13.661610Z","shell.execute_reply.started":"2022-07-12T13:13:13.419043Z","shell.execute_reply":"2022-07-12T13:13:13.660821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plotting Residuals","metadata":{}},{"cell_type":"code","source":"test_data[\"no_of_sunspots\"].values[0]","metadata":{"execution":{"iopub.status.busy":"2022-07-12T13:18:04.041476Z","iopub.execute_input":"2022-07-12T13:18:04.042122Z","iopub.status.idle":"2022-07-12T13:18:04.049583Z","shell.execute_reply.started":"2022-07-12T13:18:04.042048Z","shell.execute_reply":"2022-07-12T13:18:04.048554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data[\"no_of_sunspots\"][0]","metadata":{"execution":{"iopub.status.busy":"2022-07-12T13:16:57.977946Z","iopub.execute_input":"2022-07-12T13:16:57.978333Z","iopub.status.idle":"2022-07-12T13:16:58.000357Z","shell.execute_reply.started":"2022-07-12T13:16:57.978301Z","shell.execute_reply":"2022-07-12T13:16:57.998198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_resids = [test_data[\"no_of_sunspots\"].values[i] - preds_test[i] for i in range(len(test_data))]\n\n### Plotting the kdeplot \nsns.kdeplot(test_resids)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T13:19:01.652286Z","iopub.execute_input":"2022-07-12T13:19:01.652783Z","iopub.status.idle":"2022-07-12T13:19:01.872957Z","shell.execute_reply.started":"2022-07-12T13:19:01.652753Z","shell.execute_reply":"2022-07-12T13:19:01.872044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Plotting histogram \nsns.distplot(test_resids)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T13:19:42.965765Z","iopub.execute_input":"2022-07-12T13:19:42.966439Z","iopub.status.idle":"2022-07-12T13:19:43.199671Z","shell.execute_reply.started":"2022-07-12T13:19:42.966401Z","shell.execute_reply":"2022-07-12T13:19:43.198554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observations:\n* The test set residuals follow an almost normal distibution\n* Still, the trend component hasn't been perfectly captured and the model  ","metadata":{}},{"cell_type":"code","source":"### Check for presence of lags in autocorrelation and partial autocorrelation plots \nplot_acf(val_data[\"resid\"], lags = 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_pacf(val_data[\"resid\"], lags = 10)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T11:09:17.632833Z","iopub.execute_input":"2022-07-12T11:09:17.633230Z","iopub.status.idle":"2022-07-12T11:09:17.992320Z","shell.execute_reply.started":"2022-07-12T11:09:17.633196Z","shell.execute_reply":"2022-07-12T11:09:17.990853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Power Transformation ","metadata":{}},{"cell_type":"code","source":"### Add the code for boxcox transformation ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observations\n1. Box-Cox transformation = yields values in a lower range but it doesn't smoothen out the data much \n2. The only thing that works in our favour is that the range of values has reduced that would aid in the efficient learning of a model ","metadata":{}},{"cell_type":"code","source":"### Using box-cox transformations to identify the ideal value for lambda. A simple transformation won't work here. Let the model find the ideal transformation for us \n#### It is worthy to note that the transformation only considerspositive values. Sincde tehre are several zeroes as values, we need to add 1 to the entire dataset and then pass it to the boxcox function\ntrain_data[\"sunspots_transform\"], lamb = boxcox(train_data[\"no_of_sunspots\"]+1)\ntrain_data[\"sunspots_transform\"] = boxcox(train_data[\"no_of_sunspots\"]+1, lmbda = lamb)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Separate out the independent and dependent variables. Let the lagged values be the independent variable and the actual no. of sunspots as the dependent variable \n# for i in range(len(val_values)):\n#     ar_model = AutoReg(history, lags = 6).fit()\n#     yhat = ar_model.predict(start = len(train_values) + i + 1, end = len(train_values) + len(val_values) + i + 1, dynamic = False)\n#     preds.append(yhat)\n#     history.append(yhat)\n# X_train, y_train = train_data_persist[\"no_of_sunspots_lagged\"].values, train_data_persist[\"no_of_sunspots\"].values\n# history = [x for x in train_data_persist[\"no_of\"]]\n# X_val, y_val = val_data_persist.iloc[:, 1].values, val_data_persist.iloc[:, 0]\n# ar_model = AutoReg(y_train, lags=6, trend=\"ct\")\n# ar_model.fit(X_train, y_train)\n# preds = ar_model.predict(val_data_persist[\"no_of_sunspots\"])","metadata":{"execution":{"iopub.status.busy":"2022-07-12T06:50:52.032819Z","iopub.execute_input":"2022-07-12T06:50:52.033214Z","iopub.status.idle":"2022-07-12T06:50:52.039026Z","shell.execute_reply.started":"2022-07-12T06:50:52.033176Z","shell.execute_reply":"2022-07-12T06:50:52.037822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Reading in the data and basic formatting**","metadata":{}},{"cell_type":"code","source":"sunspots = pd.read_csv('https://raw.githubusercontent.com/jbrownlee/Datasets/master/monthly-sunspots.csv', sep=',', parse_dates=True)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T02:47:02.187242Z","iopub.status.idle":"2022-07-12T02:47:02.187749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sunspots.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T02:47:02.189001Z","iopub.status.idle":"2022-07-12T02:47:02.189509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observations","metadata":{}},{"cell_type":"code","source":"sunspots['Month'] = pd.to_datetime(sunspots['Month'])\nsunspots.set_index('Month')","metadata":{"execution":{"iopub.status.busy":"2022-07-12T02:47:02.190512Z","iopub.status.idle":"2022-07-12T02:47:02.191003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sunspots.tail()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T02:47:02.192099Z","iopub.status.idle":"2022-07-12T02:47:02.192584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualization of a time-series, and summary stats and diagnostics","metadata":{}},{"cell_type":"code","source":"med_value = sunspots['Sunspots'].median()\nquant_25 = sunspots['Sunspots'].quantile(0.25)\nquant_99 = sunspots['Sunspots'].quantile(0.99)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T02:47:02.193654Z","iopub.status.idle":"2022-07-12T02:47:02.194133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Setting up the style of background grid\nfig = plt.figure()\nplt.style.use('fivethirtyeight')\nax = sunspots['Sunspots'].plot(color='blue', fontsize=10, figsize=(10, 8))\n\n\n#A horizontal span \n#Also there are \nax.axvspan(quant_25, quant_99, color='green', alpha=0.3)\n\n#A vertical span\nax.axhspan(30, 150, color='red', alpha=0.3)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T02:47:02.195140Z","iopub.status.idle":"2022-07-12T02:47:02.195619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking if there are any null values\nsunspots.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T02:47:02.196674Z","iopub.status.idle":"2022-07-12T02:47:02.197159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There aren't any null values in the dataframe","metadata":{}},{"cell_type":"markdown","source":"**Window Functions:**\n1. Used to identify sub-periods, calculates sub-metrics of sub-periods.\n2. Rolling - same size and sliding\n3. Expanding - includes all previous values","metadata":{}},{"cell_type":"code","source":"#Setting the index back to datetime format\nsunspots = sunspots.set_index('Month')","metadata":{"execution":{"iopub.status.busy":"2022-07-12T02:47:02.198236Z","iopub.status.idle":"2022-07-12T02:47:02.198737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Rolling mean visualizations of some section of data, say from 1749 to 1753, unable to understand the concept\nsunSome_part = sunspots[1749 : 1753]\n\n\nsunSome_mean1 = sunSome_part.rolling(1).mean() #rolling mean for a single month\nsunSome_mean2 = sunSome_part.rolling(2).mean() #rolling mean for 2 consecutive months\nsunSome_mean3 = sunSome_part.rolling(3).mean() #rolling mean for a period of 3 months\n\n# ax = sunSome_mean1.plot()\n# ax.set_xlabel('Date')\n# ax.set_ylabel('Rolling Mean Variation')\n# ax.set_title('SPOT statistics')\n\nplt.style.use('fivethirtyeight')\nfig, ax = plt.subplots(1, figsize=(10, 5))\n\nax.plot(sunSome_mean1, sunSome_mean1.index, linewidth=2, markersize=12, color='green')\nax.plot(sunSome_mean2, sunSome_mean2.index, linewidth=2, markersize=12, color='blue')\nax.plot(sunSome_mean3, sunSome_mean3.index, linewidth=2, markersize=12, color='red')\n\nplt.title('Plotting out the rolling mean statistics')\nplt.xlabel('Susnpots Mean')\nplt.ylabel('Period')\nplt.legend(['1Day', '2Day', '3Day'])\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T02:47:02.199789Z","iopub.status.idle":"2022-07-12T02:47:02.200254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's plot a more compact representation of our data. Here we will be computing rolling avergae for a lagging period of 2 months, i.e, 60 days.\n#ma variable is for moving avergaes\nma = sunspots.rolling(window=2).mean()\nmstd = sunspots.rolling(window=2).std()\n\n#Adding the lower bound\nma['Lower'] = ma['Sunspots'] - (2 * mstd['Sunspots'])\n\n#Adding the upper bound\nma['Upper'] = ma['Sunspots'] + (2 * mstd['Sunspots'])\n\n#Plot the dataframe and set the labels\nplt.figure(figsize=(10, 5))\n\nax = ma.plot(linewidth=0.8, fontsize=6)\nplt.xlabel('Date')\nplt.ylabel('Number of sunspots')\nplt.xlabel('Date')\nplt.title('Rolling mean and variance of the number of sunspots over the given period of time')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T02:47:02.201419Z","iopub.status.idle":"2022-07-12T02:47:02.201907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Plotting aggregate values of a time series**","metadata":{}},{"cell_type":"code","source":"#For our use-case, let's try to plot the aggregate values for the number of spots in the year 1750\nsunspots_1750 = sunspots.iloc[sunspots.index.year==1750, : ]\nindex_month = sunspots.index.month\nsunspots_1750_by_month = sunspots.groupby(index_month).Sunspots.mean()\n\nsunspots_1750_by_month.plot()\nplt.ylabel('Cases per month')\nplt.legend('Sunspots', loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T02:47:02.203010Z","iopub.status.idle":"2022-07-12T02:47:02.203523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hence from the above visualization we can infer that the number of sunspots is at peak during the summer months,  which could be predetermindedly hypothesised.","metadata":{}},{"cell_type":"markdown","source":"Summarizing and plotting summary statistics","metadata":{}},{"cell_type":"code","source":"#Describing the dataframe\nprint(sunspots.describe())\n\n#Minmimum value\nprint(sunspots['Sunspots'].min())\n\n#Maximum value\nprint(sunspots['Sunspots'].max())","metadata":{"execution":{"iopub.status.busy":"2022-07-12T02:47:02.204550Z","iopub.status.idle":"2022-07-12T02:47:02.205026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Printing out the boxplot for visualizing summary statistics\nboxplot = sunspots.boxplot()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T02:47:02.206065Z","iopub.status.idle":"2022-07-12T02:47:02.206757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sunspots_copy = sunspots.copy()\nsunspots_copy.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T02:47:02.207763Z","iopub.status.idle":"2022-07-12T02:47:02.208854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sunspots_copy.rename(columns={'Month':'Date'}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T02:47:02.210680Z","iopub.status.idle":"2022-07-12T02:47:02.211236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Printing out the boxplot for visualizing summary statistics\nboxplot = sunspots_copy.boxplot()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T02:47:02.212373Z","iopub.status.idle":"2022-07-12T02:47:02.212887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#We observe there are outliers in the upper half of boxplot. Let's reassign the outlier values to be equal to the upper half of the boxplot.\nupper_perc = sunspots_copy['Sunspots'].quantile(0.75)\nlower_perc = sunspots_copy['Sunspots'].quantile(0.25)\n\nupper_limit = upper_perc + (3 * upper_perc)\nlower_limit = lower_perc - (3 * lower_perc)\n\nprint(upper_limit)\nprint(lower_limit)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T02:47:02.213995Z","iopub.status.idle":"2022-07-12T02:47:02.214538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#If the value of skewness is above 1, then it means there exists a positive skewness.\nsunspots_copy['Sunspots'].skew()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T02:47:02.215648Z","iopub.status.idle":"2022-07-12T02:47:02.216168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Histograms and Kernel Density Estimations(KDE):","metadata":{}},{"cell_type":"code","source":"sunspots['Sunspots'].plot(kind='hist', bins=100)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T02:47:02.217254Z","iopub.status.idle":"2022-07-12T02:47:02.217772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In practice, histograms can be a substandard method for assessing the distribution of your data because they can be strongly affected by the number of bins that have been specified. Instead, kernel density plots represent a more effective way to view the distribution of your data. An example of how to generate a density plot of is shown below:","metadata":{}},{"cell_type":"code","source":"ax = sunspots['Sunspots'].plot(kind='density', linewidth=2)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T02:47:02.218761Z","iopub.status.idle":"2022-07-12T02:47:02.219259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Since the distribution isn't normal, we will perform the following the quantile-based imputation\n#sunspots_copy[sunspots_copy['Sunspots'] > upper_limit] = upper_limit","metadata":{"execution":{"iopub.status.busy":"2022-07-12T02:47:02.220397Z","iopub.status.idle":"2022-07-12T02:47:02.220887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#There are a lot of negative values in the dataset. We need to remove them for the transformation to happen\n#sunspots_copy['Sunspots-1'] = sunspots_copy[sunspots_copy['Sunspots'] > 0]","metadata":{"execution":{"iopub.status.busy":"2022-07-12T02:47:02.221867Z","iopub.status.idle":"2022-07-12T02:47:02.222561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Log-trasnform for removing left skewness\n#sunspots_copy['Sunspots-1'] = np.log1p(sunspots_copy['Sunspots-1'])","metadata":{"execution":{"iopub.status.busy":"2022-07-12T02:47:02.223757Z","iopub.status.idle":"2022-07-12T02:47:02.224249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sunspots_copy['Sunspots-1'].plot(kind='hist', bins=100)\n#plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T02:47:02.225279Z","iopub.status.idle":"2022-07-12T02:47:02.225810Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sunspots_copy.drop(['Sunspots'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T02:47:02.226959Z","iopub.status.idle":"2022-07-12T02:47:02.227601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sunspots_copy['Sunspots-1'].plot(kind='kde')","metadata":{"execution":{"iopub.status.busy":"2022-07-12T02:47:02.228573Z","iopub.status.idle":"2022-07-12T02:47:02.229059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plotting autocorrelation and autocorrelation","metadata":{}},{"cell_type":"code","source":"#Plotting autocorrelation\n#Lags and alpha are the only important parameters in these plots\nfig = plot_acf(sunspots_copy['Sunspots'], lags=20)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T02:47:02.230116Z","iopub.status.idle":"2022-07-12T02:47:02.230619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plotting partial autocorrelation\n#Lags and alpha are the only important parameters in these plots\nfig = plot_pacf(sunspots_copy['Sunspots'], lags=20)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T02:47:02.231733Z","iopub.status.idle":"2022-07-12T02:47:02.232263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Like autocorrelation, the partial autocorrelation function also measures the correlation coefficient between a time series and a lagged version of itself. But\nthe main difference between the two is that PACF smoothens(lessens variations) the effect of lags beyond the ones explicitly mentioned.","metadata":{}},{"cell_type":"markdown","source":"**Time Series Decomposition** - for visualizing trend, seasonality and noise","metadata":{}},{"cell_type":"code","source":"rcParams['figure.figsize'] = 11, 9\n\ndecomposition = seasonal_decompose(sunspots['Sunspots'])\nfigure = decomposition.plot()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T02:47:02.233361Z","iopub.status.idle":"2022-07-12T02:47:02.233849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(dir(decomposition))","metadata":{"execution":{"iopub.status.busy":"2022-07-12T02:47:02.234984Z","iopub.status.idle":"2022-07-12T02:47:02.235519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(decomposition.seasonal)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T02:47:02.236558Z","iopub.status.idle":"2022-07-12T02:47:02.237059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Time Series decomposition is a powerful tool to reveal the structure in a time-series.","metadata":{}},{"cell_type":"code","source":"#A seasonal component(cyclic component) exists when a time-series is influenced by seasonal factors. \ndecomp_seasonal = decomposition.resid\nax = decomp_seasonal.plot(figsize=(14, 10))\nax.set_xlabel('Date')\nax.set_ylabel('Seasonality')\nax.set_title('Seasonal values of the time series')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T02:47:02.238184Z","iopub.status.idle":"2022-07-12T02:47:02.238701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So far we have known:\n1. Visualize aggregates of time series data\n2. Extract statistical summaries\n3. Autocorrelation and Partial autocorrelation\n4. Time Series decomposition.","metadata":{}},{"cell_type":"markdown","source":"Multiple time-****series plots - refer the course","metadata":{}},{"cell_type":"markdown","source":"**Also you can print out the relationships between different time series data using heatmaps and clustered heatmaps.**\n1. Create facetted plots and graphs(using the pandas .plot function and setting up the layout of plots\n2. Set horiziontal/vertical lines/regions to specify/highlight some important year/date. This is ideal for a multiple time-series dataset.\n3. Aggregate plots are also ideal for a time-series dataset. (Monthly or yearly trends) alongwith bbox_to_anchor)\n4. Seasonal decomposition of multiple time-series togther.","metadata":{}},{"cell_type":"markdown","source":"Multiple time-series visualizations and code templates","metadata":{}},{"cell_type":"markdown","source":"> Time-Series Visualizations","metadata":{}},{"cell_type":"code","source":"# Plot all time series in the jobs DataFrame\n# ax = jobs.plot(colormap='Spectral', fontsize=6, linewidth=0.8)\n    \n# Set labels and legend\n# ax.set_xlabel('Date', fontsize=10)\n# ax.set_ylabel('Unemployment Rate', fontsize=10)\n# ax.set_title('Unemployment rate of U.S. workers by industry', fontsize=10)\n# ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n# Annotate your plots with vertical lines\n# ax.axvline('2001-07-01', color='blue', linestyle='--', linewidth=0.8)\n# ax.axvline('2008-09-01', color='blue', linestyle='--', linewidth=0.8)\n\n# Show plot\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T02:47:02.239768Z","iopub.status.idle":"2022-07-12T02:47:02.240259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract the seasonal values for the decomposition of each time series\n# for ts in jobs_names:\n#     jobs_seasonal[ts] = jobs_decomp[ts].seasonal\n    \n# Create a DataFrame from the jobs_seasonal dictionary\n# seasonality_df = pd.DataFrame.from_dict(jobs_seasonal)\n\n# Remove the label for the index\n# seasonality_df.index.name = None\n\n# Create a faceted plot of the seasonality_df DataFrame\n# seasonality_df.plot(subplots=True,\n#                    layout=(4, 4),\n#                    sharey=False,\n#                    fontsize=2,\n#                    linewidth=0.3,\n#                    legend=False)\n\n# Show plot\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T02:47:02.241828Z","iopub.status.idle":"2022-07-12T02:47:02.242320Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get correlation matrix of the seasonality_df DataFrame\n# seasonality_corr = seasonality_df.corr(method='spearman')\n\n# Customize the clustermap of the seasonality_corr correlation matrix\n# fig = sns.clustermap(seasonality_corr, annot=True, annot_kws={\"size\": 4}, linewidths=.4, figsize=(15, 10))\n# plt.setp(fig.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n# plt.setp(fig.ax_heatmap.xaxis.get_majorticklabels(), rotation=90)\n# plt.show()\n\n# Print the correlation between the seasonalities of the Government and Education & Health industries\n# print(0.89)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T02:47:02.243617Z","iopub.status.idle":"2022-07-12T02:47:02.244110Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Unlabelling the indices\n#The packages to be used are pandas, numpy, statsmodels and scipy for linear regression.","metadata":{"execution":{"iopub.status.busy":"2022-07-12T02:47:02.245095Z","iopub.status.idle":"2022-07-12T02:47:02.245593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Probable questions to be asked\n#1. Do I need to resaqmple in my use-case?\n#2. Do I need to apply percent changes in my use-case?\n#3. ","metadata":{"execution":{"iopub.status.busy":"2022-07-12T02:47:02.246913Z","iopub.status.idle":"2022-07-12T02:47:02.247418Z"},"trusted":true},"execution_count":null,"outputs":[]}]}